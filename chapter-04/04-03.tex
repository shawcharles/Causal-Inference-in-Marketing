\section{Identification with Staggered Timing}
\label{sec:staggered-identification}

A retailer rolls out a loyalty programme to different regions over two years. What assumptions are needed to identify the causal effect? And when might those assumptions fail? This section articulates the identification assumptions for staggered adoption, discusses their plausibility in marketing settings, and clarifies when alternative strategies are required.

Identification of cohort-time effects $\text{ATT}(g, t)$ requires assumptions about how treated and control units would have evolved in the absence of treatment and about which units can serve as valid controls for which cohorts and periods. Two types of control units are available in staggered designs: \index{never-treated}\textit{never-treated} units ($G_i = \infty$) that remain untreated throughout the observation window, and \index{not-yet-treated}\textit{not-yet-treated} units ($G_i > t$) that will eventually be treated but have not yet adopted in period $t$. The distinction matters: never-treated units provide a stable control group but may be systematically different from treated units, while not-yet-treated units are more similar to treated units but may exhibit anticipation effects as their adoption date approaches. Figure~\ref{fig:staggered-grid} illustrates the cohort-time structure and event-time alignment in staggered designs.

\subsection*{Parallel Trends for Staggered Designs}

The parallel trends assumption for staggered adoption asserts that units adopting at different times would have followed similar trajectories in the absence of treatment. Formally, for all cohorts $g$ and $g'$ and all periods $t$ where neither cohort is yet treated ($t < g$ and $t < g'$),
\[
\mathbb{E}[Y_{it}(0) - Y_{i,t-1}(0) \mid G_i = g] = \mathbb{E}[Y_{it}(0) - Y_{i,t-1}(0) \mid G_i = g'].
\]
This assumption extends the canonical parallel trends assumption to multiple cohorts. It does not require that cohorts have the same levels or growth rates in pre-treatment periods, only that the period-to-period changes would have been the same across cohorts absent treatment. Crucially, this assumption must hold across \textit{all} cohort pairs, not just between treated and never-treated units. If early adopters (cohort $g=2$) would have grown faster than late adopters (cohort $g=6$) absent treatment, then using cohort $g=6$ as a control for cohort $g=2$ introduces bias even if both cohorts have parallel trends with never-treated units.

A stronger version of parallel trends asserts that treated cohorts and never-treated units would have followed similar trajectories:
\begin{assumption}[Strong Parallel Trends]
\label{assump:strong-parallel-trends}
For all $t=2,\dots,T$ and all cohorts $g$:
\[
\mathbb{E}[Y_{it}(0) - Y_{i,t-1}(0) \mid G_i = g] = \mathbb{E}[Y_{it}(0) - Y_{i,t-1}(0) \mid G_i = \infty].
\]
\end{assumption}
This assumption is sufficient for identifying $\text{ATT}(g, t)$ by comparing cohort $g$ to never-treated units in period $t$. In marketing settings, strong parallel trends is often implausible: never-treated units may be systematically different from treated units. Stores that never receive a loyalty programme may be in declining markets, have different management, or face different competitive pressures. The assumption that they would have followed the same trajectory as treated stores requires justification. Modern heterogeneity-robust estimators can use not-yet-treated units as controls, which requires only that cohorts would have followed similar trends up to the point at which the later cohort adopts treatment---a weaker and often more credible assumption.

\subsection*{Overlap and Support}

Overlap and support requirements ensure that comparisons between treated and control units are valid. For identification using never-treated units as controls, we require that never-treated units exist and that they are comparable to treated units on observables. If all units eventually adopt treatment (so there are no never-treated units), identification must rely on not-yet-treated controls, which requires staggered adoption with sufficient variation in timing. If treated units are systematically different from never-treated units---for example, if the programme is rolled out first to high-performing stores and never-treated stores are persistently low-performing---then parallel trends between treated and never-treated units may be implausible. Alternative approaches such as conditioning on covariates (conditional parallel trends) or using factor models (Chapters~\ref{ch:factor}, \ref{ch:advanced-matrix}) may be required.

Assessing overlap requires examining the distribution of pre-treatment covariates across treated and control groups. Standardised mean differences (SMDs) quantify covariate imbalance: as a practical rule of thumb, values around or above 0.1--0.2 standard deviations warrant attention, with larger values (above 0.25) suggesting more serious imbalance that may threaten parallel trends. Propensity score distributions reveal whether treated and control units occupy the same region of covariate space. If never-treated units cluster in a different part of the covariate distribution than treated units, comparisons between them are extrapolations rather than interpolations, and the parallel trends assumption is doing heavy lifting. Chapter~\ref{ch:design-diagnostics} provides detailed guidance on balance diagnostics.

\subsection*{No Anticipation}

The \index{no anticipation}no anticipation assumption asserts that potential outcomes in period $t$ do not depend on treatment assignments in future periods $s > t$. Anticipation can arise if units learn about impending treatment and adjust behaviour in advance. For example, if customers anticipate the launch of a loyalty programme and delay buying to qualify for rewards, then pre-treatment outcomes are contaminated by anticipation, violating no anticipation and biasing estimates.

Anticipation is not binary: it can be partial and heterogeneous. Some units may anticipate more than others---informed insiders versus uninformed customers, large firms with market intelligence versus small firms without. Some outcomes are more susceptible to anticipation than others: purchases can be delayed, but brand awareness cannot be "saved up." The degree of anticipation may also vary with the time horizon: units may not anticipate treatment six months in advance but may anticipate it one month in advance.

Diagnostic evidence for anticipation comes from event-study specifications with pre-treatment leads (Section~\ref{sec:event-dynamics}): if leads are non-zero and statistically significant, anticipation may be present. However, non-zero pre-treatment leads are observationally equivalent to differential pre-trends---both produce the same pattern in the data. Anticipation reflects behaviour change in response to expected treatment, while differential pre-trends reflect underlying trajectory differences unrelated to treatment expectations. Distinguishing them requires institutional knowledge: if units could not have known about impending treatment, non-zero leads indicate differential pre-trends rather than anticipation. When anticipation is plausible, the analysis should estimate anticipatory effects explicitly rather than assuming them away.

\subsection*{SUTVA and Spillovers}

As introduced in Chapter~\ref{ch:frameworks}, the stable unit treatment value assumption (SUTVA) asserts that potential outcomes for unit $i$ do not depend on the treatment assignments of other units. SUTVA is routinely violated in marketing through spillovers, network effects, and competitive interactions. A loyalty programme offered to customers in one store may generate word-of-mouth that influences buying at nearby stores. Advertising in one market may spill over to adjacent markets through media overlap. One firm's pricing decision may trigger competitive responses that alter outcomes for rival firms.

Spillovers complicate DiD identification in several ways. Positive spillovers from treated to control units---such as word-of-mouth or demonstration effects---contaminate control outcomes upward, biasing the DiD estimate toward zero (underestimation of the direct effect). Negative spillovers from treated to control units---such as competitive displacement or cannibalisation---contaminate control outcomes downward, biasing the DiD estimate away from zero (overestimation of the direct effect). The direction of bias depends on the sign and magnitude of the spillover, and in practice both positive and negative spillovers may operate simultaneously. If spillovers are strong, the assumption that not-yet-treated or never-treated units provide valid counterfactuals breaks down, and estimates conflate direct effects with spillover effects.

Design-based solutions to spillovers include defining clusters that internalise spillovers (so that treatment and spillover effects occur within the same cluster and are estimated jointly) and creating buffer zones that separate treated and control units geographically or along other dimensions (so that spillovers dissipate before reaching controls). When spillovers are central to the research question, explicit spillover models (Chapter~\ref{ch:spillovers}) can estimate direct and spillover effects separately by specifying exposure mappings that describe how units' treatments affect other units' outcomes. These models require additional assumptions and data (for example, knowledge of the network structure or the geographic adjacency matrix). However, they avoid the biases that arise from ignoring spillovers.

\subsection*{Factor Structure Relaxations}

Factor structure relaxations provide an alternative when standard parallel trends is implausible but units are subject to common time-varying shocks with differential exposure. Interactive fixed effects models (Chapters~\ref{ch:factor}, \ref{ch:advanced-matrix}) posit that untreated potential outcomes can be decomposed as
\[
Y_{it}(0) = \sum_{r=1}^R \lambda_{ir} f_{tr} + \varepsilon_{it},
\]
where $R$ is the number of latent factors, $f_{tr}$ are factors common to all units in period $t$, and $\lambda_{ir}$ are unit-specific loadings that capture differential exposure to each factor. This low-rank representation can absorb unit and time fixed effects into the factors and accommodates differential trends driven by common shocks---macroeconomic conditions, industry demand shifts, platform algorithm changes---without requiring that period-to-period changes are identical across units. Identification relies on a low-rank assumption: $R$ is small relative to the minimum of $N$ and $T$, so that the factors and loadings can be estimated from the untreated observations and used to impute counterfactual outcomes for treated observations.

Factor models are particularly valuable when treated units are few, heterogeneous, and embedded in a panel with many control units and periods. A single treated market launching a new product, a single platform entering a city, or a single brand adopting a new advertising strategy can all be analysed using factor models if comparable control units are available and if the factor structure provides a more credible counterfactual than parallel trends.

Factor models assume that the factor structure is exogenous to treatment: the factors $f_{tr}$ and loadings $\lambda_{ir}$ are not affected by treatment itself. If treatment changes the factor structure---for example, if a platform entry creates a new competitive dynamic that alters how units respond to macroeconomic shocks---then the imputed counterfactuals are invalid. Factor models can also fail if the factors are correlated with treatment timing (so that units adopting early have systematically different loadings than units adopting late), or if the pre-treatment period is too short to estimate the factors reliably. The cost of factor models is the need to estimate the factors and loadings, which requires a rich pre-treatment period and may be sensitive to the choice of $R$ or to deviations from the low-rank assumption. Factor models are not a panacea for parallel trends violations; they trade one set of assumptions for another.

DiD assumptions should be justified using institutional knowledge, graphical and statistical diagnostics, and targeted sensitivity analyses rather than taken on faith. Pre-period evidence and placebo tests can lend support to parallel trends, even though they can never prove it. Pre-treatment leads help to detect anticipation, while knowledge of likely spillover channels and attenuation patterns informs whether SUTVA is plausible. When adopting factor structures, examine pre-period fit carefully and experiment with different numbers of factors. Finally, vary control sets, windows, and estimators to check whether substantive conclusions survive reasonable perturbations. With this identification toolkit in hand, we now turn to the mechanics of two-way fixed effects and their pitfalls under heterogeneity.