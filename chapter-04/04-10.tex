\section{Workflow Checklist}
\label{sec:did-workflow}

This section provides a compact end-to-end protocol for conducting DiD analyses in marketing panels. The workflow integrates the estimand specification, diagnostic assessments, estimator selection, inference procedures, and reporting standards developed in this chapter and cross-referenced to related chapters.

\textbf{Define estimands and map cohorts.} Begin by clarifying the substantive question and defining the target estimand precisely. Is the goal to estimate the overall ATT (average effect on all treated units and periods), event-time effects $\theta_k$ (dynamic profile of effects over time since adoption), cohort-specific effects $\tau_g$ (effects for early vs late adopters), or calendar-time effects $\tau_t$ (effects in specific periods accounting for time-varying shocks)? Align the estimand to the business question. Use overall ATT for go/no-go decisions, event-time effects for understanding dynamics and carryover, and cohort-specific effects for targeting future rollouts.

Map adoption cohorts by creating a cohort-time matrix: rows for units, columns for periods, cells indicating cohort $g$ (the first period in which each unit is treated) or never-treated status ($g = \infty$). Visualise the adoption pattern to confirm that there is sufficient variation in timing and that comparison groups (never-treated or not-yet-treated units) are available for all treated cohorts and periods.

\textbf{Choose estimators and pre-specify aggregations.} Based on the estimand and the data structure, select one or more modern DiD estimators: Callaway--Sant'Anna for flexible aggregation and overall ATT, Sun--Abraham for event-time effects and pre-trend testing, de Chaisemartin--d'Haultf\oe uille for diagnostics and sensitivity to TWFE, or Borusyak--Jaravel--Spiess for factor-structure settings. Estimate TWFE as a benchmark. Do not rely on it as the primary estimate unless diagnostics confirm that heterogeneity is absent.

Pre-specify the aggregation weights: will event-time effects be weighted by cohort size, by treated unit-periods, or uniformly? Will the overall ATT weight cohorts equally or by sample size? Document these choices in a pre-analysis plan or analysis script to ensure transparency and to prevent ex post cherry-picking of aggregations that produce favourable results.

\textbf{Specify event-time windows and binning.} Decide how many pre-treatment and post-treatment event times to include. Longer pre-treatment windows provide more pre-trend evidence but require cohorts to have long pre-periods. Longer post-treatment windows trace long-run dynamics but may be unavailable if some cohorts adopt late. Choose a reference period (typically $k = -1$) and specify binning (if needed) to ensure adequate sample size in each event-time bin.

\textbf{Run pre-trend and placebo diagnostics.} Estimate an event-study specification with multiple pre-treatment leads. Test whether pre-treatment coefficients are jointly zero using a Wald test with cluster-robust standard errors (not a standard F-test, which assumes homoskedasticity). Plot event-study coefficients with confidence intervals to visually inspect for pre-trends. Conduct placebo-in-time tests using only pre-treatment data and check that placebo estimates are near zero. Conduct placebo-in-units tests using never-treated units as placebo-treated and check that estimates are near zero.

If pre-trends are present or placebo estimates are large, consider alternative identification strategies: conditioning on covariates (estimate propensity scores and check balance), factor models (Chapter~\ref{ch:factor}), or synthetic control (Chapter~\ref{ch:sc}). If diagnostics reveal violations of parallel trends or other assumptions, return to Step 1 to reconsider the estimand definition or identification strategy.

\textbf{Assess covariate balance and overlap.} Compute standardised mean differences (SMDs) for key covariates across treated cohorts and control units. As a practical rule of thumb, values around or above 0.1--0.2 standard deviations warrant attention, with 0.2 often used as a simple benchmark, though the appropriate threshold depends on how strongly covariates predict outcomes. If imbalances are large, adjust for covariates using regression, propensity score weighting, or matching, and re-check balance after adjustment.

Plot propensity score distributions for treated and control units. Trim extreme propensity scores (for example, below 0.1 or above 0.9) if overlap is limited. Document the trimming rule and report the fraction of the sample excluded.

\textbf{Choose clustering and inference procedures.} Cluster standard errors by unit as a default. Consider two-way clustering (by unit and time) if cross-unit correlation within periods is plausible. If the number of clusters is very small (fewer than 20), prefer wild cluster bootstrap or randomisation inference; with 20--50 clusters, compare asymptotic cluster-robust standard errors to bootstrap results, as in Section~\ref{sec:did-inference}; with 50 or more clusters, asymptotic cluster-robust standard errors are generally reliable. If the design is experimental, use randomisation inference to respect the randomisation protocol.

If testing multiple event-time coefficients, subgroup effects, or sensitivity analyses, decide on a multiplicity adjustment: Bonferroni for conservative family-wise error rate control, FDR for less conservative control of false discoveries, or Romano--Wolf stepdown for better power while controlling family-wise error rate. Distinguish primary from exploratory analyses. Apply multiplicity adjustments to primary tests. Report exploratory tests without adjustment but clearly labelled as exploratory.

\textbf{Estimate and report aggregated effects.} Estimate the chosen modern DiD estimator(s) and aggregate cohort-time effects $\tau(g, t)$ into the pre-specified summary measures: overall ATT, event-time effects $\{\theta_k\}$, cohort-specific effects $\{\tau_g\}$, or calendar-time effects $\{\tau_t\}$. Report point estimates, standard errors, confidence intervals, and p-values.

Plot event-study graphs showing $\theta_k$ against $k$ with confidence intervals. Highlight the reference period ($k = -1$) and annotate pre-treatment and post-treatment regions. Provide tables summarising overall ATT, cohort-specific effects, and aggregations of interest.

Compare estimates across methods (CS, SA, BJS, TWFE) to assess sensitivity to estimator choice. If estimates agree, conclusions are robust. If estimates diverge, investigate the sources of divergence (weights, comparison groups, factor structure) and report results from multiple methods with discussion of which is most credible and why.

\textbf{Conduct sensitivity analyses.} Vary the set of control units (never-treated only vs never-treated and not-yet-treated), the event-time window (short vs long), the binning choices (fine vs coarse), and the covariate adjustments (no controls vs rich controls). Construct a specification curve showing the distribution of estimates across specifications. Report the median, interquartile range, and range of estimates, and discuss whether conclusions are stable.

Conduct leave-one-cohort-out and leave-one-period-out analyses. Exclude each cohort or period in turn, re-estimate, and check whether the overall ATT changes substantially. If a single cohort or period drives the result, investigate why and assess robustness.

If spillovers are plausible, estimate models that exclude nearby units, define buffer zones, or estimate explicit spillover effects (Chapter~\ref{ch:spillovers}). Compare estimates with and without spillover adjustments to bound the bias.

\textbf{Document and report transparently.} Prepare a report that includes the research question, the data structure (panel dimensions, adoption pattern, sample characteristics), the estimand definition, and the chosen estimators and aggregation weights. Document the diagnostic results (pre-trends, placebo tests, covariate balance), the primary estimates (overall ATT, event-time effects), and the sensitivity analyses (specification curve, leave-one-out, spillover adjustments). Report the inference procedures (clustering, bootstrap, multiplicity) and provide substantive interpretation.

Register the analysis plan ex ante if possible, or timestamp the analysis script and document deviations from the initial plan. Provide replication materials: cleaned data (or simulated data if proprietary), analysis scripts, and documentation of software versions and packages used. This enables readers to verify results and to conduct alternative analyses.

By following this workflow, practitioners can conduct DiD analyses that are transparent, rigorous, and aligned with modern best practices. The workflow integrates design-based reasoning, diagnostic testing, and sensitivity analysis, ensuring that conclusions are credible and that assumptions are articulated and assessed. The result is causal evidence that withstands scrutiny and that informs strategic decisions with confidence.

This chapter has developed the difference-in-differences framework from its canonical 2Ã—2 form through modern methods for staggered adoption with heterogeneous treatment effects. We have defined estimands that respect heterogeneity, explained the pitfalls of two-way fixed effects under heterogeneity, surveyed modern heterogeneity-robust estimators, developed diagnostic workflows, and illustrated applications in marketing. Chapter~\ref{ch:event} develops event-study designs in more detail, tracing dynamic treatment paths and testing for anticipation effects.
\index{difference-in-differences|)}
