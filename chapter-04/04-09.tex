\section{Marketing Applications and Patterns}
\label{sec:did-marketing}

Marketing panels feature rich adoption patterns, substantial heterogeneity, and pervasive threats to identification assumptions. This section illustrates how the DiD framework applies to common marketing applications: loyalty programme rollouts, retail pricing policy changes, and platform channel expansions. We emphasise the estimand definitions, the identification challenges, the choice of estimator, and the diagnostic and inference considerations specific to each setting.

\subsection*{Loyalty Programme Rollouts with Staggered Adoption}

A retail chain operates 500 stores observed over 12 quarters. The chain launches a loyalty programme in staggered fashion: 100 stores in quarter three, 200 stores in quarter five, 150 stores in quarter seven, and 50 stores never receive the programme during the sample. Outcomes are quarterly sales per store. The goal is to estimate the effect of the programme on sales and to trace how effects evolve as customers enrol, accumulate points, and develop habits.

The data structure is a balanced panel with $N = 500$ and $T = 12$, a thin panel well suited to modern DiD estimators. The adoption pattern creates three treated cohorts ($g = 3, 5, 7$) and a never-treated control group ($g = \infty$). The primary estimand is the overall ATT: the average effect on sales for treated stores and quarters relative to the counterfactual of no programme. Secondary estimands include event-time effects $\theta_k$ (to trace dynamics) and cohort-specific effects $\tau_g$ (to assess whether early adopters experience different effects than late adopters).

Identification relies on parallel trends: in the absence of the programme, treated and control stores would have experienced similar trends in sales. This assumption is plausible if the rollout timing is driven by operational capacity or by store identifiers (for example, alphabetical order) rather than by anticipated sales trends. If the programme is rolled out first to high-growth stores, parallel trends is violated. Diagnostics include plotting pre-treatment trends for each cohort, estimating an event-study specification with leads to test for pre-trends, and checking covariate balance (store size, demographics, competitive intensity) across cohorts.

Spillovers are a serious concern. Customers may refer friends, creating positive spillovers from treated stores to nearby control stores. Customers may cross-shop, shifting spend from non-programme stores to programme stores. Positive spillovers attenuate the estimated effect because control store outcomes are elevated by spillovers from treated stores, reducing the observed difference. Negative spillovers (e.g., treated stores cannibalising sales from control stores) would inflate the estimated effect. The direction of bias depends on the sign and magnitude of spillovers, which should be assessed using explicit spillover models or geographic discontinuities. Design-based solutions include defining clusters that group nearby stores, creating buffer zones (excluding stores within a certain radius of treated stores from the control group), or estimating explicit spillover models (Chapter~\ref{ch:spillovers}) that quantify direct and indirect effects.

Heterogeneity is expected. The programme may work well in affluent, low-competition areas where customers have high lifetime value and respond strongly to rewards. It may have negligible effects in saturated urban markets where customers are less loyal. Event-time effects are likely to grow over time as customers accumulate points and develop habits, so the immediate effect $\theta_0$ understates the long-run impact. Modern DiD estimators (Callaway--Sant'Anna or Sun--Abraham) are appropriate because they allow effects to vary across cohorts and over time. TWFE would likely produce misleading estimates due to heterogeneity and because already-treated stores would serve as implicit controls for newly treated stores.

Inference should cluster by store to account for serial correlation. With 500 store-level clusters, standard cluster-robust standard errors are generally reliable (see Section~\ref{sec:did-inference}), but most of the identifying variation comes from only three treated cohorts. Wild cluster bootstrap provides a valuable robustness check, especially if those cohorts differ sharply from the never-treated group. If inference is instead conducted at the cohort level, treating cohorts as clusters, the effective number of clusters is very small, and permutation-style inference at the cohort level may be more appropriate. Event-time estimates require multiplicity adjustment if the goal is to test each $\theta_k$ individually, but if event-time estimates are reported as exploratory evidence and the primary test is on the overall ATT, no adjustment is needed.

\subsection*{Retail Pricing Policy Changes with Staggered Implementation}

A retailer implements a new pricing policy (for example, everyday low pricing replacing frequent promotions) across product categories in waves over six quarters. The retailer observes sales, margins, and customer visit frequency for 200 categories over 24 weeks. The policy is implemented in 50 categories in week 4, 75 categories in week 8, 50 categories in week 12, and 25 categories never adopt the policy during the sample. The goal is to estimate the effect on margins and to assess whether effects differ by category characteristics (price elasticity, brand concentration, competitive intensity).

The data structure is a panel with $N = 200$ categories and $T = 24$ weeks, a thin panel with moderate $T$. The adoption pattern creates four cohorts ($g = 4, 8, 12, \infty$). The primary estimand is the overall ATT on margins. Secondary estimands include event-time effects (to trace dynamics) and heterogeneous effects by category characteristics (to guide targeting of future policy changes).

Identification relies on parallel trends across categories. This assumption is plausible if the rollout is driven by operational constraints (for example, staggering implementation to allow staff training and system updates) rather than by anticipated margin trends. If the policy is rolled out first to high-margin categories or to categories with declining trends, parallel trends is violated. Diagnostics include pre-trend tests by cohort and placebo tests using never-treated categories.

Competitive responses are a key threat. If a retailer reduces promotional frequency in one category, competitors may respond by increasing promotions in that category, partially offsetting the margin gains. Alternatively, customers may substitute across categories, shifting spend from treated categories to untreated categories with frequent promotions. These general equilibrium effects mean that the estimated effect conflates the direct effect of the policy with indirect effects due to substitution and competition. Addressing these requires explicit modelling of cross-category substitution patterns or estimation of spillover effects (Chapter~\ref{ch:spillovers}).

Heterogeneity by category characteristics can be explored using subgroup analyses or interactions. Estimate $\tau(g, t)$ separately for high-elasticity and low-elasticity categories, or include interactions between treatment indicators and category characteristics in the regression. Causal forests (Chapter~\ref{ch:ml-nuisance}) provide a flexible approach to estimating heterogeneous effects as a function of many covariates simultaneously.

Inference should cluster by category. With 200 category-level clusters, standard cluster-robust standard errors are typically adequate, but again most of the identifying variation comes from a small number of treated cohorts. Wild cluster bootstrap is therefore recommended as a robustness check, particularly if one or two cohorts appear unusually influential. Two-way clustering (by category and week) accounts for potential cross-category correlation within weeks due to common demand shocks. Multiple testing adjustments are needed if testing heterogeneous effects across many subgroups or category characteristics.

\subsection*{Platform Channel Expansion with Phased Entry}

A food delivery platform enters 30 cities over two years, with entry times staggered based on market size, regulatory environment, and operational capacity. The platform observes monthly restaurant revenues in 50 cities (30 treated, 20 never-treated) over 24 months. The goal is to estimate the effect of platform entry on restaurant revenues and to assess whether effects differ by restaurant type (independent vs chain, cuisine type, price point).

The data structure is a panel with $N = 50$ cities and $T = 24$ months. The adoption pattern creates many cohorts (up to 30 if each city enters at a different time) and a never-treated control group of 20 cities. The primary estimand is the overall ATT on restaurant revenues. Secondary estimands include event-time effects (to trace the speed of market penetration and competitive adjustment) and heterogeneous effects by restaurant type.

Identification relies on parallel trends across cities. This assumption is questionable if the platform enters larger, faster-growing cities first. This creates selection on observables (market size, growth rate) and possibly on unobservables (unobserved demand shocks, entrepreneurial activity). Conditioning on city characteristics through covariate adjustment or propensity score weighting can restore conditional parallel trends. Alternatively, factor models (Chapter~\ref{ch:factor}) can accommodate differential trends driven by common shocks (macroeconomic conditions, pandemic waves, national restaurant trends) without requiring parallel trends in levels.

Competitive responses are central. Incumbent platforms adjust pricing and marketing in response to entry, partially offsetting the treatment effect. Restaurants respond by adjusting menus, delivery fees, and participation decisions. These dynamic adjustments mean that the effect in the first month post-entry differs from the effect after the market reaches a new equilibrium. Event-time estimates trace these dynamics, and long-run effects (large $k$) capture the equilibrium impact.

The overall ATT on restaurant revenues may mask substantial heterogeneity: platform entry may increase revenues for some restaurants (those that gain visibility and delivery capacity) while decreasing revenues for others (those that lose foot traffic or face new competition). Decomposing effects by restaurant type is essential for understanding the distributional consequences of platform entry.

Spillovers across cities are plausible if the platform's national advertising or if migration and commuting create cross-city linkages. Geographic buffer zones (excluding cities within a certain distance from treated cities) or explicit spatial models (Chapter~\ref{ch:spillovers}) can address these spillovers. General equilibrium effects---whether entry creates new demand or merely redistributes existing demand among restaurants---require comparing total restaurant revenues (treated and untreated restaurants) to assess category expansion.

Heterogeneous effects by restaurant type can be explored using subgroup analyses. Estimate effects separately for independent restaurants and chain restaurants, for high-priced and low-priced restaurants, and for different cuisines. If the platform is more effective for certain types of restaurants, this guides future targeting and partnership strategies.

Inference should cluster by city. With $G = 30$ cohorts, asymptotic cluster-robust standard errors are generally adequate (per the guidance in Section~\ref{sec:did-inference}), though wild cluster bootstrap provides a robustness check. Two-way clustering by city and month accounts for cross-city correlation within months due to pandemic phases, policy changes, or national trends. Multiple testing adjustments are needed if testing effects for many restaurant types or subgroups.

These three applications illustrate the versatility of modern DiD methods and the importance of tailoring the analysis to the substantive context. The choice of estimand, the diagnostic workflow, the estimator selection, and the inference procedure all depend on the data structure, the adoption pattern, the threats to identification, and the business question. Transparent reporting and sensitivity analyses ensure that conclusions are credible and that readers understand the assumptions required for causal interpretation. The following workflow checklist synthesizes the methods and diagnostics developed in this chapter into a practical step-by-step protocol.
