\section{ML Integration Beyond Nuisance}
\label{sec:ml-beyond-nuisance}

Chapter~\ref{ch:ml-nuisance} treated machine learning as a black box for estimating nuisance functions. But modern ML---particularly foundation models---offers something more: mechanisms to encode unstructured data that traditional econometrics ignores. Text, images, and network structure all carry information about confounders we cannot directly measure. This frontier demands new rules for when ML features are safe to use in causal pipelines.

\subsection*{Embeddings as Proximal Controls}

Marketing data brims with unstructured confounders. A product's description affects both its pricing and its conversion rate. We might ignore the text entirely, or create manual dummies---neither approach captures the richness of what customers actually read. Large language models offer an alternative: map the text into dense vectors $Z \in \mathbb{R}^d$ and use these embeddings as controls.

Conceptually, embeddings act as proxies for unobserved confounders $U$ in the sense of proximal causal inference \citep{miao2018identifying, tchetgen2020introduction}. If the embedding dimension is large enough and the mapping rich enough, $Z$ may span the latent space of $U$. \citet{veitch2020adapting} formalise this for text, showing how BERT representations can adjust for semantic confounding that no structured variable captures.

The danger lies in how we learn $Z$. Two strategies exist, and they differ sharply in their causal implications. Frozen embeddings---off-the-shelf models like BERT or ResNet trained on Wikipedia or ImageNet---capture semantic meaning without peeking at our outcome. They are exogenous to our treatment mechanism, which makes them safer for causal adjustment. Fine-tuned embeddings are riskier. When we tune an encoder on our marketing dataset to improve prediction, the encoder sees the outcome (or variables correlated with it). The resulting embedding becomes a collider. Controlling for it induces bias---what practitioners call ``leakage.''

The engineering rule follows directly. Treat representation learning as part of the nuisance estimation pipeline. If we must fine-tune, we must cross-fit the representation itself: train the encoder on Fold A, encode features for Fold B, and estimate effects on Fold B. The target label $Y$ must never influence the features $Z$ used to predict it, unless strictly separated by cross-fitting.

\subsection*{High-Dimensional Treatments and Generative AI}

Generative AI introduces treatments that are inherently high-dimensional. An ad is not just ``Treatment A vs B,'' but a specific image generated from a latent vector. We cannot estimate the causal effect of every unique image---positivity fails because no two users see the exact same pixel configuration. This is the core challenge of what \citet{fong2016discovery} and \citet{keith2020text} call text-as-treatment estimation.

We can make progress by reframing the estimand. Instead of asking about the effect of a specific generated image (which may never appear again), we view the prompt as defining a stochastic intervention. The treatment becomes the distribution $P(\text{Content} | \text{Prompt})$. The estimand becomes the expected outcome under a shift in this distribution---closer to a policy evaluation than a point treatment effect.

To operationalise this, we must map the high-dimensional treatment space back to something we can estimate. One approach clusters the generative latent space into discrete concepts---``Minimalist'' versus ``Cluttered,'' say---and estimates the average treatment effect of these clusters. Another projects the high-dimensional treatment onto interpretable basis functions (text embeddings of the prompt, for instance) and estimates a causal effect function over the prompt space. Both strategies sacrifice some fidelity to gain identification.

\subsection*{Causal Representation Learning}

The ultimate goal is causal representation learning \citep{scholkopf2021toward}: learning valid causal variables from low-level inputs (pixels, tokens) without supervision. Standard supervised learning disentangles factors based on correlation. Causal learning seeks to disentangle factors based on invariance across environments.

For marketing, this means moving beyond ``predicting churn with embeddings'' to ``learning a representation of user loyalty that is invariant to discount depth.'' We want features that capture stable structural relationships, not features that happen to correlate with outcomes in our particular dataset. This remains an open engineering challenge, but the direction is clear: representations must be judged not by their predictive accuracy, but by their stability under intervention.

\subsection*{Open Problems}

Three problems define this frontier. First, when is a BERT embedding ``sufficient'' to block confounding? We lack tests for proxy sufficiency in high dimensions. The identification results from proximal causal inference assume we know something about the relationship between proxies and latent confounders---assumptions that are hard to verify when $Z$ is a 768-dimensional vector.

Second, adaptive content creates propensity nightmares. When generative AI produces content dynamically based on user history, the treatment assignment mechanism becomes a complex function of that history. Recovering the propensity score $\pi(X)$ requires access to the generative model's internal logic (its logits), not just its output. Most marketing teams do not have this access.

Third, controlling for a high-dimensional vector ``blocks confounding'' but offers no insight into what was confounded. Was it product quality? Sentiment? Topic? We need methods that project bias corrections back onto interpretable axes---methods that tell us not just that we adjusted for something, but what that something was.
