\section{Partial Identification and Sensitivity}
\label{sec:partial-identification}

Point identification requires assumptions that are often contestable. Parallel trends may not hold exactly, as discussed in Chapters~\ref{ch:frameworks}--\ref{ch:threats}. SUTVA may be violated by spillovers (Chapter~\ref{ch:spillovers}). Outcomes may be measured with error (Chapter~\ref{ch:data-measurement}). When these assumptions fail, point estimates are biased. Partial identification offers an alternative: rather than assuming violations away, we bound the estimand under explicit assumptions about the magnitude of violations. This section sketches the open problems in making partial identification practical for marketing panels.

\subsection*{Why Partial Identification?}

Point estimates with narrow confidence intervals can create false confidence when identification assumptions are uncertain. Bounds acknowledge that we do not know the exact effect but can rule out implausible values. They also make assumptions explicit. Rather than assuming parallel trends hold exactly, we might assume they hold within $\pm \delta$ per period and report results for specific values of $\delta$. Readers can then assess whether those values are plausible in their context.

The sensitivity analyses and breakdown diagnostics from Chapters~\ref{ch:inference} and~\ref{ch:design-diagnostics} show how estimates change when assumptions are perturbed. Partial identification complements these tools by turning perturbations into explicit bounds on the estimand. Instead of saying that "results are robust to X", the analyst specifies the magnitude of X under which conclusions hold and the range of effects consistent with larger violations.

\subsection*{Bounding Strategies by Violation Type}

Different assumption violations call for different bounding approaches. If parallel trends hold within $\pm \delta$ per period, a simple template bounds the ATT by the point estimate plus or minus $T_1 \cdot \delta$, where $T_1$ is the number of post-treatment periods. Calibrating $\delta$ from pre-treatment trend differences or from domain knowledge is essential but often ad hoc.

When spillovers are present, individual treatment effects are not point identified, but bounds can be constructed under assumptions about spillover magnitude. If spillovers affect outcomes by at most $\gamma$, the direct effect lies within the observed effect $\pm \gamma$. Classical measurement error attenuates estimates toward zero; when the error variance is known or can be bounded, we can construct corresponding bounds. Non-classical error requires stronger structure.

Rosenbaum bounds\index{Rosenbaum bounds} (Definition~\ref{def:rosenbaum-gamma} and Proposition~\ref{prop:rosenbaum-bounds} in Chapter~\ref{ch:threats}) assess unmeasured confounding by asking how strong a hidden confounder would need to be to overturn the result. The sensitivity parameter $\Gamma$ quantifies the required confounding strength. When outcomes are missing not at random, Manski bounds provide worst-case intervals by imputing extreme values for missing observations. These intervals can be wide but require no assumptions about the missing-data mechanism.

\subsection*{Current Tools and Limitations}

Chapters~\ref{ch:inference} and~\ref{ch:design-diagnostics} develop sensitivity analyses that show how estimates move under perturbations of assumptions. These are valuable but often lack defaults for what perturbation magnitudes to consider. Specification curves in Section~\ref{sec:synthesis-reporting} report estimates across a range of specifications, revealing sensitivity but not delivering formal bounds on the estimand.

Breakdown analysis reports the violation magnitude at which the substantive conclusion would changeâ€”for example, "the effect remains positive unless parallel trends are violated by more than 5\% per period." This is informative but requires choosing a threshold for what counts as a changed conclusion. Partial identification methods are less well implemented than point estimation. Practitioners rarely have ready-to-use tools for constructing bounds in DiD, SC, or factor-based designs.

\subsection*{Combining Multiple Violations}

In practice, multiple assumptions may be violated simultaneously. Parallel trends and SUTVA may both be uncertain, and measurement error may be present as well. Bounds must then account for several types of violations at once. Joint bounds are wider than bounds for any single violation. Optimising over multiple violation parameters to find the implied range of the estimand is computationally intensive, especially when the parameter space is high dimensional.

Specifying plausible magnitudes for multiple violations requires domain knowledge. There is no consensus on how to elicit or report these magnitudes in marketing settings. The analyst must make judgement calls that shape the reported bounds and, in turn, the apparent strength of the evidence.

\subsection*{Reporting Standards}

Transparent reporting of partial identification requires stating the assumed bounds on parallel-trends deviations, spillover magnitudes, or confounding strength. Vague statements such as "robust to violations" should be avoided. The point estimate under the maintained assumptions should be reported alongside bounds under explicit violations so that readers can see both the best guess and the range of uncertainty.

Breakdown analysis should report the violation magnitude at which conclusions change and relate this to observable features where possible. For instance, calibrating allowable trend deviations from pre-treatment gaps or calibrating spillover magnitudes from observed cross-market effects grounds the bounds in evidence rather than arbitrary choices.

\subsection*{Open Problems}

Several questions remain open. How should we choose default violation magnitudes when domain knowledge is limited? Data-driven calibration methods are underdeveloped. How do we construct and report bounds under multiple simultaneous violations without the bounds becoming uninformatively wide? Efficient algorithms for computing bounds under complex violation structures are needed.

How do we integrate partial identification with the design diagnostics from Chapter~\ref{ch:design-diagnostics}? Diagnostics that fail should trigger bounds, not just warnings. How do we communicate bounds to non-technical audiences? Wide bounds may be misinterpreted as "we do not know anything" rather than "we know the effect lies in this range."

\subsection*{Research Directions}

Progress will require practical templates for bounded parallel trends, SUTVA violations, and measurement error, with default calibration strategies. We need software tools that implement partial identification for common panel estimators such as DiD, SC, and factor models. Joint sensitivity analysis that efficiently computes bounds under multiple violations would make partial identification usable in practice. Reporting standards that require bounds alongside point estimates, with explicit violation magnitudes, would improve transparency. Communication strategies for presenting bounds to business audiences in actionable terms would bridge the gap between methodological rigour and practical decision-making.

Until these advances materialise, practitioners should report sensitivity to key assumptions, state explicit violation magnitudes, and acknowledge when point estimates rest on contestable assumptions.
