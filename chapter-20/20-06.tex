\section{Robust and Distribution-Free Inference}
\label{sec:robust-inference-outlook}

Chapter~\ref{ch:inference} developed robust inference methods---randomisation inference, conformal prediction, and bootstrap procedures---that avoid strong distributional assumptions. These methods deliver exactly what marketing analysts need: valid uncertainty quantification without betting on normal errors or correctly specified variance models. Yet panels pose challenges that standard implementations do not address. This section maps the frontier where theory meets practice.

\subsection*{Randomisation and Permutation Inference}

Randomisation inference delivers exact finite-sample p-values under the sharp null by comparing an observed statistic to its distribution over treatment assignments. The appeal is conceptual purity: we make no distributional assumptions beyond the assignment mechanism itself. In panels, however, we face three obstacles that limit this purity.

The first is computational. With $N$ units and $T$ periods, the number of possible assignments grows rapidly. Even under staggered adoption, exact enumeration becomes infeasible for panels larger than a few dozen units. Monte Carlo approximations work in practice but introduce sampling error that undermines the ``exactness'' we sought.

The second is structural. When units cluster---stores within DMAs, users within social networks---permutations must respect cluster boundaries. Permuting at the cluster level reduces the effective sample size, often to fewer than 50 permutations, which limits p-value precision to at best 0.02. This granularity may be too coarse for many applications.

The third is identification. Randomisation inference requires knowledge of the assignment mechanism. When treatment is observational or adaptive, as in Sections~\ref{sec:adaptive-experimentation} and Chapter~\ref{ch:design-diagnostics}, we rarely know the true mechanism. The theoretical guarantee of exactness then rests on a foundation we cannot verify.

\subsection*{Mosaic Inference: Local Exchangeability in Panels}

Recent work by \citet{spector2025mosaic} offers a way forward. The key insight is that panels have a \textit{mosaic structure}: even when global exchangeability fails, local exchangeability may hold within certain blocks. Consider a staggered adoption design. Pre-treatment periods for eventually-treated units are exchangeable with pre-treatment periods for never-treated units, even though post-treatment periods are not. We can permute within these locally exchangeable blocks while respecting the non-exchangeability across blocks.

This mosaic approach recovers valid permutation inference for panels without requiring global exchangeability. The analyst identifies which observations are ``similar enough'' to permute---typically based on treatment timing and covariate strata---and constructs tests that exploit this local structure. The resulting p-values are exact within each block and can be combined across blocks using established methods.

The practical implication is significant. We no longer need to choose between exact inference on tiny samples and approximate inference on large ones. Mosaic methods scale to large panels while preserving finite-sample validity where the structure permits. The cost is that we must correctly specify which blocks are locally exchangeable---a design judgment that requires careful thought about the treatment assignment process.

\subsection*{Conformal Prediction for Counterfactuals}

Conformal prediction provides distribution-free prediction intervals with finite-sample coverage guarantees. Extending these ideas to causal inference is attractive but faces two obstacles: panel dependence and counterfactual estimation.

For synthetic controls, \citet{chernozhukov2021exact} provide exact inference by permuting residuals, exploiting the estimator's structure rather than raw outcome exchangeability.

For general heteroskedastic panels, \textbf{Conformalized Quantile Regression (CQR)} \citep{romano2019conformalized} offers a robust alternative. Instead of assuming constant variance (as in standard conformal prediction), CQR estimates the conditional quantiles $Q_\alpha(Y|X)$ using quantile regression (e.g., a quantile random forest) and then calibrates these estimates using a holdout set to guarantee coverage. This adapts interval width to the local difficulty of prediction---wider intervals during volatile periods, narrower ones during stable periods. This adaptivity is crucial for marketing panels where variance often scales with the mean (e.g., sales volatility is higher during holidays).

\subsection*{Bootstrap Methods for Dependent Data}

Bootstrap methods resample data to approximate sampling distributions. Their flexibility makes them the workhorse of applied inference, but panels require care in how we resample.

Block bootstrap preserves temporal dependence by resampling contiguous time blocks rather than individual observations. The block length trades off bias from short blocks (which break dependence) against variance from long blocks (which yield few resamples). Optimal block length depends on the dependence structure, which we rarely know. In practice, we try several lengths and check sensitivity.

Cluster bootstrap preserves within-cluster dependence by resampling entire clusters. With many clusters, this works well. With few clusters---common in marketing where we might have 20 DMAs or 15 retail chains---the bootstrap distribution is poorly approximated. The problem is not sample size per se but the number of independent units we resample from.

Wild cluster bootstrap addresses the few-clusters problem by multiplying residuals by random weights rather than resampling observations. This expands the effective permutation space while respecting cluster structure. Implementation requires specifying the weight distribution (Rademacher weights are standard) and the residual type (HC2 or HC3 adjustments help with leverage). The wild cluster bootstrap is now our preferred method when clusters number between 10 and 50.

When data exhibit both temporal and cross-sectional dependence, no single bootstrap is fully satisfactory. Hybrid approaches that resample in both dimensions are computationally demanding and theoretically underdeveloped. Recent work by \citet{almeida2025estimating} provides variance estimators for causal panel estimators that can guide bootstrap implementation, but a unified framework remains elusive.

\subsection*{Practical Guidance}

Given the limitations above, how should practitioners proceed? We offer the following guidance, recognising that all choices involve trade-offs.

For analytical standard errors, cluster-robust methods remain the baseline. Cluster at the level of treatment assignment---if stores are randomised within DMAs, cluster at the store level, not the DMA level. When in doubt, cluster conservatively (at a higher level), accepting wider confidence intervals as the price of validity.

For few-cluster settings (fewer than 20 clusters), use wild cluster bootstrap with Rademacher weights. Report both analytical cluster-robust SEs and bootstrap confidence intervals. If they diverge substantially, investigate why---the divergence itself is diagnostic.

For temporal dependence, consider block bootstrap with multiple block lengths. If results are sensitive to block length, report the range and acknowledge the uncertainty. HAC standard errors (Newey-West) provide an analytical alternative but require bandwidth selection with similar trade-offs.

For permutation tests, use mosaic inference when you can identify locally exchangeable blocks. Pre-treatment periods across treatment groups are typically a safe choice. With fewer than 50 effective permutations, report exact p-values to two decimal places and acknowledge the granularity.

For conformal inference, the \citet{chernozhukov2021exact} approach works for synthetic control settings. For other panel designs, conformal methods remain experimental. Use them to supplement, not replace, other inference methods.

\subsection*{Research Directions}

Three developments would advance the field. First, we need implementations of mosaic inference that scale to large panels and provide guidance on block specification. The theory exists; the software does not. Second, we need conformal methods for heterogeneous treatment effects in panels---methods that provide conditional coverage under dependence. Third, we need bootstrap procedures that handle both temporal and cross-sectional dependence with clear guidance on when each dominates.

Until these advances become standard, practitioners should use multiple inference methods and report sensitivity to assumptions. Agreement across methods is reassuring. Disagreement is informative---it tells us that conclusions depend on modelling choices we cannot resolve with the data at hand.
