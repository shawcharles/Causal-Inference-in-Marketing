\section{Specification: Leads and Lags}
\label{sec:event-specification}

Specifying an event-study regression requires choosing which event-time indicators to include (\index{leads and lags}leads and lags), which event time to normalise to zero (the \index{reference bin}reference bin), how to group extreme event times (\index{binning}binning), and how long a window to estimate (\index{window selection}window selection). These choices affect identification, precision, and interpretation. We provide practical guidance grounded in potential outcomes and design-based diagnostics.

\subsection*{Basic Specification}

The standard \index{two-way fixed effects}\index{TWFE|see{two-way fixed effects}}TWFE event-study regression takes the form
\[
Y_{it} = \alpha_i + \beta_t + \sum_{k \in \mathcal{K} \setminus \{-1\}} \theta_k^{\text{TWFE}} \mathbb{I}\{t - G_i = k\} + \varepsilon_{it},
\]
where $\alpha_i$ are unit fixed effects, $\beta_t$ are time fixed effects, and $\mathcal{K}$ is the set of event times included in the window. The coefficients $\theta_k^{\text{TWFE}}$ are regression coefficients, not causal parameters; under treatment effect heterogeneity, they may differ substantially from the true event-time effects $\theta_k$ due to contamination from already-treated units serving as implicit controls. $\theta_{-1}^{\text{TWFE}}$ is normalised to zero. Under heterogeneity, $\theta_k^{\text{TWFE}}$ can differ from the causal $\theta_k$ due to contamination bias (Chapter~\ref{ch:did}, Sections~\ref{sec:twfe-pitfalls}--\ref{sec:modern-estimators}).

To resolve this, modern estimators use interaction-weighted specifications (for example, \index{estimators!Sun--Abraham}Sun--Abraham) or aggregation of \index{group-time effects}group-time effects (for example, \index{estimators!Callaway--Sant'Anna}Callaway--Sant'Anna). The interaction-weighted specification is:
\[
Y_{it} = \alpha_i + \beta_t + \sum_{g} \sum_{k \neq -1} \delta_{g,k} \mathbb{I}\{G_i=g\} \mathbb{I}\{t - G_i = k\} + \varepsilon_{it}.
\]
This fully saturates the model with cohort-specific event-time coefficients $\delta_{g,k}$, using never-treated or not-yet-treated units as the comparison group (the choice depends on the estimator and the available data; see Chapter~\ref{ch:did}). The target parameter $\theta_k$ is then estimated by aggregating these clean coefficients:
\[
\hat{\theta}_k = \sum_g \hat{\omega}_{g,k} \hat{\delta}_{g,k},
\]
where the weights $\hat{\omega}_{g,k}$ are typically proportional to cohort size at event time $k$ (sample-size weights) or uniform across cohorts (equal weights). The choice of weights is a substantive decision that affects the estimand (Section~\ref{sec:event-estimands}). This ensures that $\theta_k$ represents a properly weighted average of causal effects, not an opaque regression-weighted artefact of TWFE.

The omitted category should be a pre-treatment period, not a post-treatment period. Omitting a post-treatment period would interpret all coefficients as deviations from a treated baseline, obscuring the causal effect. For example, if $k = 0$ were omitted, $\theta_1$ would estimate the difference between $k = 1$ and $k = 0$, which is the change in the effect from the first to the second post-treatment period, not the effect relative to an untreated baseline. The convention of omitting $k = -1$ ensures that post-treatment coefficients $\theta_k$ for $k \geq 0$ measure the treatment effect relative to the immediate pre-treatment period, which aligns with the canonical DiD interpretation.

Some researchers omit $k = 0$ (the treatment period itself) and interpret post-treatment effects relative to the immediate post-treatment baseline. This can be appropriate if the treatment is implemented part-way through the period, so that outcomes at $k = 0$ are a mix of treated and untreated observations. It is also appropriate when anticipation contaminates the treatment period itself: if units respond to expected treatment before it formally begins, $k = 0$ may already reflect anticipatory behaviour, making it a poor baseline. However, omitting $k = 0$ complicates interpretation because the effect at $k = 1$ is then the difference from $k = 0$, not from a purely untreated baseline. The cleanest approach is to omit $k = -1$ and interpret $\theta_0$ as the immediate effect, $\theta_1$ as the effect one period post-treatment, and so on, all relative to the pre-treatment baseline.

\subsection*{Binning}

Binning groups extreme event times into intervals to stabilise estimates when data are sparse at the tails. If the panel spans many periods and some cohorts adopt early, observations at large event times ($k \gg 0$ or $k \ll 0$) may be few, leading to imprecise estimates and wide confidence intervals. Binning aggregates these sparse observations into a single bin. This is a bias-variance trade-off: binning reduces variance (tighter confidence intervals) but can introduce bias if effects vary within the bin. If effects at $k = 8$ and $k = 12$ differ substantially, pooling them into a single bin produces a coefficient that represents neither. For example, a binning scheme might define bins $k \in \{-10, -9, -8\}$ (aggregated as $k \leq -8$), $k \in \{-7, -6, \ldots, -2\}$ (separate coefficients for each), $k = -1$ (omitted), $k \in \{0, 1, 2, \ldots, 10\}$ (separate coefficients), and $k \in \{11, 12, 13, \ldots\}$ (aggregated as $k \geq 11$).

The choice of where to bin depends on the support of the data and the substantive interest in distinguishing effects at different event times. If the goal is to test for pre-trends, the pre-treatment window ($k < 0$) should include enough separate bins to detect divergence. Bins far from treatment ($k \ll 0$) can be aggregated if support is thin. If the goal is to estimate long-run effects, the post-treatment window should extend as far as the data allow. Bins at large $k$ can be aggregated if the effect has stabilised or if observations are sparse. Binning should be pre-specified based on the support of the data (reported in a support table or figure) and should be transparent in reporting. Sensitivity analyses that vary the binning scheme provide evidence on whether conclusions are robust to aggregation choices.

\begin{tcolorbox}[colback=orange!5!white,colframe=orange!50!black,title=Endpoint Binning Caveat]
Endpoint bins ($k \leq -L$ or $k \geq K$) pool observations at heterogeneous event times into a single coefficient, mixing early and late dynamics. This pooling obscures information about effect evolution and can bias interpretation. For example, a coefficient for $k \geq 8$ averages effects at $k = 8$, $k = 12$, and $k = 20$, which may differ substantially if effects decay or grow over time. Interpret endpoint bins as averages over heterogeneous horizons, not as point estimates at specific event times. Report sensitivity analyses excluding endpoint bins or using alternative cutoffs to assess robustness. When the endpoint bin dominates the sample (many observations fall into $k \geq K$), consider extending the window or reporting the full disaggregated profile.
\end{tcolorbox}

\subsection*{Window Selection}

Window selection determines how many pre-treatment and post-treatment event times to include in the regression. Symmetric windows include equal numbers of leads and lags (for example, $k \in \{-5, \ldots, 5\}$), while asymmetric windows may include more lags than leads if the primary interest is in post-treatment dynamics. The window should be chosen to balance several considerations. Coverage: the window should span the period over which effects are expected to evolve. Support: the window should not extend so far that many event times have few observations. Statistical power: wider windows include more observations and improve precision, but they also increase the number of parameters estimated. Diagnostic value: the pre-treatment window should be long enough to detect pre-trends, typically at least three to five pre-treatment periods.

\begin{tcolorbox}[colback=orange!5!white,colframe=orange!50!black,title=Pre-Trend Testing Trade-Off]
Longer pre-treatment windows provide more power to detect pre-trends, but they also increase the risk of finding spurious pre-trends due to multiple testing (testing many $\theta_k$ for $k < 0$ increases the chance of rejecting at least one by chance). Joint tests of all pre-treatment coefficients (Section~\ref{sec:event-diagnostics}) address this by testing the null that all pre-treatment coefficients are jointly zero, controlling the family-wise error rate. When reporting individual pre-treatment coefficients, consider multiplicity adjustments or clearly label the analysis as exploratory.
\end{tcolorbox}

\subsection*{Cohort Composition and Support}

Cohort composition varies across event times, and this variation affects the interpretation of $\theta_k$. At event time $k = 0$, all cohorts contribute observations (each cohort is observed at its adoption period). At event time $k = 5$, only cohorts that adopted at least five periods before the end of the sample contribute observations. At event time $k = -5$, only cohorts that adopted at least five periods after the start of the sample contribute observations. This means that estimates at extreme event times $k$ may be identified from subsets of cohorts. If treatment effects are heterogeneous across cohorts, the composition effects can confound interpretation.

For example, suppose early adopters (cohort $g = 2$) experience large effects and late adopters (cohort $g = 8$) experience small effects. At event time $k = 0$, both cohorts contribute, and $\theta_0$ is a weighted average of their effects. At event time $k = 4$, only cohort $g = 2$ contributes (because cohort $g = 8$ does not have four post-treatment periods in the sample). Therefore $\theta_4$ reflects only the early-adopter effect. A plot of $\theta_k$ might show a growing effect over event time, but this could be driven by the changing cohort composition (early adopters dominating at large $k$) rather than by true dynamics within cohorts. Reporting the cohort composition at each event time (a table or figure showing which cohorts contribute to each $k$) and estimating cohort-specific profiles $\theta_{g,k}$ provide diagnostics for this issue.

Support tables report, for each event time $k$, the number of observations, the number of cohorts contributing, and the identities of those cohorts. Support figures plot the sample size or effective sample size (accounting for weights and clustering) as a function of $k$, highlighting where the data are rich and where they are sparse. These diagnostics inform binning and window selection and help readers assess the robustness of conclusions to the inclusion or exclusion of extreme event times.

The specification of leads and lags is not a one-size-fits-all decision but a design choice informed by the data structure, the substantive question, and the diagnostic priorities. Pre-specifying the reference period, binning scheme, and window in a pre-analysis plan disciplines the analysis and guards against data-driven specification searches that capitalise on chance. Reporting the support and cohort composition for each event time ensures transparency and enables readers to assess whether the estimated profile is driven by genuine dynamics or by composition effects.
