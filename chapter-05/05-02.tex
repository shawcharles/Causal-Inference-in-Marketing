\section{Event-Time Estimands}
\label{sec:event-estimands}

Event-time estimands define the causal quantities of interest in event-study designs. The fundamental building block is the average treatment effect at event time $k$, denoted $\theta_k$, which measures the average causal effect of treatment $k$ periods after adoption (or $k$ periods before adoption if $k < 0$). Formal definitions require care to specify the population being averaged over (all treated units, specific cohorts, or weighted combinations), the reference level relative to which effects are measured, and the aggregation scheme when treatment effects are heterogeneous across cohorts.

\subsection*{Basic Event-Time Effects}

The simplest definition treats $\theta_k$ as the average difference between treated and untreated potential outcomes for units at event time $k$. Formally, let $\mathcal{E}_k = \{(i,t) : t - G_i = k\}$ be the set of observations at event time $k$. The target parameter is:
\[
\theta_k = \frac{1}{\sum_{(i,t) \in \mathcal{E}_k} 1} \sum_{(i,t) \in \mathcal{E}_k} \mathbb{E}[Y_{it}(1) - Y_{it}(\infty) \mid G_i < \infty].
\]
Here $Y_{it}(1)$ denotes the treated potential outcome---shorthand for ``outcome under treatment''---while $Y_{it}(\infty)$ denotes the never-treated counterfactual. In the staggered-adoption notation from Chapter~\ref{ch:did}, the more precise form $Y_{it}(G_i)$ indexes the potential outcome by the unit's actual adoption period $G_i$; when $t \geq G_i$, we have $Y_{it}(1) = Y_{it}(G_i)$, so the two notations refer to the same quantity for ever-treated units. The conditioning on $G_i < \infty$ restricts to ever-treated units; never-treated units contribute to identification as controls but not to the estimand. This definition pools across all cohorts $g$ and all calendar periods $t$ such that $t - g = k$. It implicitly weights cohort-time observations by their prevalence in the sample. If treatment effects are constant across cohorts and calendar time, this simple average identifies a common causal effect. If treatment effects are heterogeneous, $\theta_k$ is a weighted average of cohort-specific and calendar-specific effects.

To make the aggregation explicit, decompose $\theta_k$ in terms of cohort-time effects $\tau(g, t)$. The event-time effect $\theta_k$ is a weighted average of $\tau(g, g + k)$ across cohorts:
\[
\theta_k = \sum_{g} \omega_{g,k} \, \tau(g, g + k), \quad \text{with } \sum_g \omega_{g,k} = 1.
\]
where the weights $\omega_{g,k}$ reflect the relative sample sizes or importance of each cohort at event time $k$. Common weighting schemes include uniform weights (each cohort receives equal weight), sample-size weights (cohorts are weighted by the number of observations at event time $k$), and treated-unit-period weights. Different weighting schemes can produce different $\theta_k$ estimates even when the underlying $\tau(g, g + k)$ are the same. As emphasised in Chapter~\ref{ch:did}, this is a design choice, not a nuisance. Transparency about weights is essential.

\subsection*{Cohort-Specific Profiles}

Cohort-specific event-time profiles, denoted $\theta_{g,k}$, estimate the effect for cohort $g$ at event time $k$ without aggregating across cohorts:
\[
\theta_{g,k} = \mathbb{E}[Y_{it}(1) - Y_{it}(\infty) \mid G_i = g, \, t - G_i = k].
\]
Note that $\theta_{g,k} = \tau(g, g+k)$: the cohort-specific event-time effect is simply the cohort-time effect from Chapter~\ref{ch:did} evaluated at calendar time $t = g + k$. Estimating cohort-specific profiles is valuable when treatment effects are expected to vary substantially across cohorts---for example, if early adopters differ from late adopters in ways that moderate the treatment response, or if macroeconomic conditions or competitive environments differ across the calendar periods during which different cohorts are treated. Cohort-specific profiles also provide a diagnostic. If profiles are similar across cohorts, pooling them into a single $\theta_k$ is defensible. If profiles diverge, aggregation obscures heterogeneity and may mislead.

Calendar-time aggregation provides an alternative to event-time aggregation when the goal is to estimate the total effect in specific calendar periods accounting for the mix of cohorts at different event times. For example, a retailer rolling out a loyalty programme in waves may want to estimate the total sales impact in quarter four, summing effects across cohorts that are at different event times in that quarter. Calendar-time aggregation weights $\tau(g, t)$ by the prevalence of each cohort in period $t$. This produces an estimate of the average treatment effect in period $t$ across all treated units observed in that period.

Event-time aggregation, by contrast, pools observations by event time $k$ regardless of calendar period, producing an estimate of the average effect $k$ periods post-adoption.

\subsection*{Cumulative and Long-Run Effects}

Long-run and cumulative effects are central to marketing applications where interventions are intended to have persistent impacts. The cumulative effect over $K$ post-treatment periods is the sum of event-time effects:
\[
\sum_{k=0}^{K} \theta_k.
\]
This sum measures the total impact of treatment from adoption through $K$ periods post-adoption, integrating both immediate and delayed effects. Note that the cumulative effect is only identified for $k$ values observed in the data. If the goal is to estimate the total long-run effect but the post-treatment window is short, the analyst must either extend the observation period or extrapolate using parametric assumptions (for example, exponential decay). Extrapolation introduces model dependence and should be reported transparently. The long-run multiplier compares the cumulative effect to the immediate effect:
\[
\text{LRM} = \frac{\sum_{k=0}^{K} \theta_k}{\theta_0}, \quad \text{assuming } \theta_0 \neq 0.
\]
If $\text{LRM} > 1$, the cumulative impact exceeds the immediate impact, indicating positive carryover or ramp-up. If $\text{LRM} < 1$, the immediate effect overstates the long-run impact, suggesting decay or erosion. Marketing interventions such as loyalty programmes, advertising campaigns, and platform launches typically exhibit $\text{LRM} > 1$ because customer habit formation, brand awareness accumulation, and network effects generate persistence and growth. Promotional pricing or temporary discounts may exhibit $\text{LRM} < 1$ if demand is merely shifted forward in time rather than created.

\begin{tcolorbox}[colback=orange!5!white,colframe=orange!50!black,title=Edge Case: Delayed-Onset Effects]
When effects ramp up gradually, $\theta_0$ may be near zero, making LRM undefined or numerically unstable. In such cases, replace the denominator with the average effect over the first $m$ post-treatment periods: $\text{LRM}^* = \sum_{k=0}^{K} \theta_k \,/\, \bigl(\frac{1}{m}\sum_{j=0}^{m-1} \theta_j\bigr)$. This modification captures ramp-up dynamics while providing a meaningful multiplier. Alternatively, report the cumulative effect $\sum_{k=0}^{K} \theta_k$ directly without normalisation. The key is transparency: if $\theta_0 \approx 0$, the standard LRM is uninformative, and readers should be shown the full event-time profile rather than a single summary statistic. See Section~\ref{sec:staggered-estimands} for the analogous issue with cohort-specific effects.
\end{tcolorbox}

The half-life of a treatment effect---the number of periods required for the effect to decay to half its peak magnitude---provides another summary of dynamics. The half-life concept assumes that effects decay monotonically after reaching a peak; if effects ramp up before decaying, the ``initial magnitude'' $\theta_0$ may not be the peak, and the half-life should be measured from the peak rather than from $k = 0$. If effects follow exponential decay from the peak, $\theta_k = \theta_{\max} \exp(-\lambda (k - k_{\max}))$ for $k \geq k_{\max}$, then the half-life is $k_{1/2} = \log(2) / \lambda$. Estimating the half-life requires either imposing parametric structure (as in distributed lag models, Chapter~\ref{ch:dynamics}) or directly reading off the event-study path at the point where $\theta_k \approx \theta_0 / 2$. The advantage of the event-study approach is transparency. The half-life is directly visible in the plot of $\theta_k$ against $k$, making it accessible to non-technical stakeholders and robust to misspecification of the functional form.

\subsection*{Aggregation and Reference Choices}

Aggregation schemes shape the interpretation and policy relevance of event-time estimates. Consider a loyalty programme rolled out to stores in three cohorts ($g = 2, 5, 8$) over eight quarters. The programme may have different effects for each cohort because early-adopting stores differ in size, demographics, or competitive intensity. Calendar-time aggregation in quarter eight weights the cohort-two stores (at event time $k = 6$), cohort-five stores (at event time $k = 3$), and cohort-eight stores (at event time $k = 0$) by their sample sizes. This produces an estimate of the total programme impact in quarter eight. Event-time aggregation at $k = 3$ weights cohorts two, five, and eight at their respective $k = 3$ observations, producing an estimate of the effect three quarters post-adoption averaged across cohorts. The choice depends on the substantive question. Calendar-time aggregation is natural for forecasting or planning decisions tied to specific periods, while event-time aggregation is natural for understanding the dynamic profile and for generalising to future rollouts.

The reference level for event-time effects is also a choice. Effects are typically reported relative to the period immediately before adoption ($k = -1$), which is normalised to zero by construction (omitting the $k = -1$ indicator from the regression). This convention interprets $\theta_k$ as the change in outcomes from $k = -1$ to $k$. Alternative reference levels are possible: effects could be reported relative to the average of all pre-treatment periods, relative to a specific pre-treatment period further in the past, or relative to the outcome level at $k = 0$. The choice of reference affects the magnitude and interpretation of coefficients but not the differences between coefficients. These differences measure contrasts between event times and are invariant to the normalisation. Transparency about the reference level and robustness checks using alternative references ensure that conclusions are not artefacts of the normalisation.

There is no single ``right'' estimand. Choose based on the decision: expansion (profiles for comparable stores), advertising ROI (cumulative effects), entry strategy (ramp-up and long-run). Estimate the full profile, then aggregate transparently into the summary that answers the question. The next section develops the regression specification.
