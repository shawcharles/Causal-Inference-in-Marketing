\section{Ex Ante Diagnostics and Placebos}
\label{sec:ex-ante-diagnostics}

Credible design-based inference requires diagnosing the plausibility of identification assumptions before treatment is assigned and outcomes are observed. \textit{Ex ante} diagnostics check whether the design satisfies the conditions required for causal identification and whether balance on pre-treatment covariates is adequate. In observational settings where treatment has already been assigned, these diagnostics are performed \textit{ex post} but remain essential for assessing credibility. These diagnostics also assess whether placebo tests using pre-treatment data or alternative outcomes support the assumptions. We outline the key \textit{ex ante} diagnostic tasks and indicate where the detailed design workflows in Chapter~\ref{ch:design-diagnostics} and inference-focused diagnostics in Chapter~\ref{ch:inference} pick up.

\subsection{Pre-Treatment Fit and Balance}

If the design relies on parallel trends, pre-treatment fit assesses whether treated and control units followed similar trajectories before treatment. Formally, using an event-study specification with pre-treatment leads $\theta_k$ for $k < 0$, we test the joint null hypothesis that all pre-treatment coefficients are zero:
\[ H_0: \theta_k = 0 \quad \forall k < 0. \]
Plotting outcome trends for treated and control groups over pre-treatment periods provides visual evidence. If trends diverge, parallel trends becomes less plausible, and alternative identification strategies---factor models (Chapters~\ref{ch:factor}, \ref{ch:advanced-matrix}) or synthetic control (Chapter~\ref{ch:sc})---may be required.

\paragraph{Power Limitation} This test has low power when pre-treatment periods are short or noisy. Failing to reject $H_0$ does not prove that parallel trends holds, only that we lack evidence against it. The test provides supportive but not definitive evidence.

Balance checks assess whether treated and control units are similar on observed covariates. We recommend the normalised difference (or standardised mean difference) as a scale-invariant measure of imbalance. For a covariate $X$, the normalised difference is:
\[ \Delta_X = \frac{\bar{X}_{1} - \bar{X}_{0}}{\sqrt{(S^2_{1} + S^2_{0})/2}}, \]
where $\bar{X}_d$ and $S^2_d$ are the sample mean and variance in group $d \in \{0,1\}$. Common thresholds are $|\Delta_X| > 0.1$ or $0.2$, though the appropriate threshold depends on how strongly the covariate predicts outcomes: a large imbalance on a covariate weakly related to outcomes may matter less than a small imbalance on a covariate that strongly predicts outcomes. Large imbalances signal that treated units differ systematically from controls, raising concerns about unobserved confounders. When balance is marginal, report both unadjusted and covariate-adjusted estimates to assess sensitivity to the imbalance.

\subsection{Placebo Tests and Negative Controls}

Placebo tests in time assign fictitious treatment dates in pre-treatment periods and estimate the treatment effect using those placebo dates. If the placebo estimates are near zero, the parallel trends assumption is supported because treated and control units evolved similarly before actual treatment. If the placebo estimates are large, pre-trends are present, violating parallel trends.

\paragraph{Power Limitation} Placebo tests have low power: failing to reject the null of zero placebo effects does not prove that parallel trends holds. Pre-treatment data may be too noisy or the pre-treatment period too short to detect violations. A non-significant placebo test provides supportive but not definitive evidence. Placebo tests are most credible when pre-specified as part of the design rather than chosen opportunistically after seeing outcomes.

Negative controls are outcomes that should not be affected by the treatment but that may be affected by confounders. For example, if the treatment is a loyalty programme targeted at frequent shoppers in grocery stores, a negative control outcome might be sales of a product category that should not respond to the programme but shares the same local economic shocks. If the loyalty programme appears to affect the negative control outcome, this suggests confounding rather than a causal effect.

\paragraph{Challenge of Finding Good Negative Controls} A good negative control must satisfy two conditions: (1) it is unaffected by the treatment, and (2) it is affected by the same confounders as the primary outcome. Finding outcomes that satisfy both conditions is difficult. If the negative control is affected by different confounders than the primary outcome, a null result on the negative control provides little reassurance. Negative controls are particularly valuable in observational studies where the assignment mechanism is not fully understood and confounding is a serious concern, and like placebo tests they are best specified \textit{ex ante} in the analysis plan.

\subsection{Overlap and Support Plans}

In designs that rely on conditional independence---treatment is as-good-as-random conditional on covariates---overlap requires that treated and control units have similar covariate distributions. If treated units have covariates that are not observed in control units (or vice versa), then extrapolation is required to estimate counterfactual outcomes, and estimates may be sensitive to functional form assumptions. \textit{Ex ante} overlap diagnostics plot propensity score distributions for treated and control units and check whether the supports overlap. Trimming observations with extreme propensity scores, using matching or weighting to reweight the sample toward regions of overlap, or explicitly modelling the outcome as a function of covariates using flexible methods (Chapter~\ref{ch:ml-nuisance}) can improve robustness when overlap is limited.

\paragraph{Trimming Changes the Estimand} Trimming observations with extreme propensity scores improves overlap but changes the target population. The estimated effect is then for the trimmed population---units with moderate propensity scores---not the original population. If units with extreme propensity scores are substantively important (for example, the most loyal customers or the largest markets), trimming may exclude precisely the units of greatest interest. Document the trimming rule and report the fraction of the sample excluded.

\subsection{Planning for Spillovers and Interference}

When SUTVA is likely to be violated---customers refer friends, competitors respond to rivals' actions, advertising spills over across markets---the design should include an exposure mapping that specifies how spillovers propagate. An exposure mapping defines, for each unit, which other units' treatments affect its outcomes. In a geographic spillover model, the exposure mapping might specify that unit $i$'s outcome depends on treatments in units within a certain radius or in adjacent markets. In a network spillover model, the exposure mapping follows the network structure: friends, followers, or co-purchasers.

Specifying the exposure mapping in advance disciplines the analysis by making the spillover assumptions explicit. It also guides data collection: if spillovers are expected to decay with distance, outcomes should be measured in buffer zones at varying distances from treated units. If network spillovers are expected, network data should be collected.

\paragraph{Misspecification Risk} If the exposure mapping is wrong---spillovers decay faster or slower than assumed, or propagate through different channels than modelled---estimates of both direct and spillover effects will be biased. When the spillover structure is uncertain, sensitivity analysis across different exposure mappings is essential. Chapter~\ref{ch:spillovers} develops estimators for direct and spillover effects under various exposure mappings, but the key insight is that spillover models should be planned \textit{ex ante} based on substantive knowledge rather than discovered \textit{ex post} through data mining.

These \textit{ex ante} diagnostic tasks should be completed before treatment assignment and data collection begin. By identifying potential threats to validity early, researchers can adapt the design, collect additional data, or adjust the analysis plan to ensure credible causal inference. The diagnostics outlined here complement the detailed design workflows in Chapter~\ref{ch:design-diagnostics}, which provide step-by-step guidance for implementing each diagnostic task.
