\section{Threats to Validity and Design Adaptation}
\label{sec:threats-validity}

Even well-designed experiments and quasi-experiments face threats to validity from confounding events, measurement issues, and violations of identifying assumptions. This section catalogues the major threats encountered in marketing panels and discusses design adaptations to mitigate them. Detailed treatments of specific threats appear in Chapter~\ref{ch:threats}, which develops diagnostics and sensitivity analyses in depth. Here we provide a design-stage checklist for anticipating and mitigating these threats. The key point is that threats should be anticipated \textit{ex ante} and addressed through design choices rather than diagnosed and repaired \textit{ex post}.

\subsection{Seasonality and Event Interference}

Marketing outcomes exhibit strong seasonal patterns driven by holidays, weather, school calendars, and cultural events. A loyalty programme launched in December may appear highly effective because sales rise during the holiday season, but the rise may reflect seasonality rather than a causal effect. Event interference occurs when major events---sporting championships, elections, natural disasters, pandemics---affect treated and control units differently during the experiment window.

Design solutions depend on whether concurrent controls are feasible. If concurrent control is feasible (for example, geo-experiments with spatial separation), align treatment and control periods seasonally: if a geo-experiment runs from January to March, the control group should also be observed from January to March in different geographies. If concurrent control is not feasible (for example, single-unit designs), lengthen the experiment to span multiple seasons and include seasonal fixed effects or detrended outcomes in the analysis. For phased rollouts spanning multiple quarters, cohorts can be stratified by launch quarter to ensure that effects are estimated within seasonal periods.

\subsection{Policy and Algorithm Changes}

Platforms frequently update their algorithms for ad delivery, search ranking, recommendation, and pricing. If an algorithm change coincides with a treatment assignment, the estimated effect conflates the treatment with the algorithm change, biasing the estimate. Policy changes---new privacy regulations, shifts in platform policies, macroeconomic interventions---create similar confounds.

Design adaptations include scheduling experiments during stable periods, coordinating with platform or policy calendars to avoid known changes, and using staggered adoption to estimate effects separately for cohorts that experience different algorithm or policy regimes. If a confounding change is unavoidable, the analysis can include indicators for the change and estimate treatment-by-change interactions, though this reduces power and complicates interpretation.

\subsection{Measurement Shifts}

Changes in measurement systems---new data sources, revised definitions, improved tracking technologies---alter the measured outcomes without reflecting true changes in underlying behaviour. For example, a switch from survey-based measurement to scanner-based measurement may increase reported sales not because actual sales increased but because scanner data have better coverage. Platform metrics such as impressions, clicks, and conversions are subject to frequent redefinitions as platforms adjust methodologies.

Design solutions include freezing measurement systems during the experiment, collecting both old and new measurements during transition periods to quantify the measurement shift, and using alternative outcomes or negative controls that should not be affected by the measurement shift to check for spurious effects. In practice, freezing measurement systems is often infeasible---platforms change metrics without notice, and researchers rarely control data collection infrastructure. When measurement shifts are unavoidable, documenting the timing and nature of the shift and conducting sensitivity analysis around the shift date are essential. Chapter~\ref{ch:data-measurement} discusses measurement issues in depth and provides methods for bounding the bias introduced by measurement error.

\subsection{Buffers and Robustness Windows}

When threats to validity are uncertain or multiple confounding factors are plausible, robustness checks provide evidence that conclusions are stable across defensible modelling choices. Buffers---periods or units excluded from the analysis---create separation between treatment and confounds. For example, excluding the first week after a treatment starts (a washout or burn-in period) mitigates contamination from anticipation or measurement lag. Excluding the last week before a treatment ends mitigates decay or announcement effects. Robustness windows vary the length of pre-treatment and post-treatment periods to check whether conclusions depend on the choice of window.

Specification curves aggregate estimates across many specifications---different control sets, different fixed effects, different time windows, different clustering choices---and show the distribution of estimates. If estimates are stable across specifications, conclusions are robust. If estimates vary widely, the choice of specification matters, and the analyst should report the full distribution rather than a single preferred estimate. Chapter~\ref{ch:design-diagnostics} develops these tools systematically.

\subsection{When Parallel Trends Is Implausible: Factor Designs}

If pre-treatment diagnostics reveal that treated and control units are on divergent trends, the parallel trends assumption is implausible. Factor models (Chapters~\ref{ch:factor}, \ref{ch:advanced-matrix}) relax parallel trends by assuming that untreated potential outcomes are generated by an interactive fixed effects structure:
\[ Y_{it}(0) = \mu_i + \xi_t + \mathbf{\lambda}_i^\top \mathbf{f}_t + \varepsilon_{it}, \]
where $\mathbf{f}_t$ is a vector of $R$ latent common factors (capturing macroeconomic trends, industry demand shifts) and $\mathbf{\lambda}_i$ are unit-specific factor loadings (capturing sensitivity to those shocks).

This structure nests the standard two-way fixed effects model as the special case with no latent factors ($R=0$), where untreated potential outcomes reduce to $Y_{it}(0) = \mu_i + \xi_t + \varepsilon_{it}$. For example, if $\mathbf{f}_t$ represents a "tech sector downturn" factor, units with high loadings $\mathbf{\lambda}_i$ will be more affected. Factor designs use the control units to estimate the span of the factors $\mathbf{f}_t$ and the pre-treatment periods of the treated unit to estimate its loadings $\mathbf{\lambda}_i$. The counterfactual is then imputed as $\hat{Y}_{it}(0) = \hat{\mu}_i + \hat{\xi}_t + \hat{\mathbf{\lambda}}_i^\top \hat{\mathbf{f}}_t$.

Identification requires a rich set of control units ($N_0$) and pre-treatment periods ($T_0$) such that $\min(N_0, T_0) \gg R$, where $R$ is the number of factors. With few control units or short pre-treatment windows, the factor structure cannot be reliably estimated. The number of factors $R$ can be selected using information criteria (BIC, AIC) or cross-validation on pre-treatment data. Sensitivity analysis across different values of $R$ is essential, as misspecifying $R$ biases the counterfactual.

\paragraph{Bias-variance Trade-off} Factor models relax parallel trends but introduce additional assumptions (low-rank structure and time-invariant factor loadings that are not affected by treatment or its anticipation) and can have higher variance than standard DiD. In simulations, factor-based estimators often have larger standard errors than DiD when parallel trends holds, but this penalty is justified when parallel trends is violated. If parallel trends is plausible, DiD is more efficient. Factor models are most valuable when pre-treatment diagnostics clearly reject parallel trends and when the panel has sufficient dimensions to estimate the factor structure reliably.

These threats to validity are not exhaustive but represent the most common challenges in marketing panel studies. The key principle is anticipation: threats should be identified and addressed through design choices rather than discovered during analysis. When threats cannot be eliminated, robustness checks and sensitivity analyses provide evidence on the stability of conclusions. Transparent reporting of design choices and threat mitigation strategies enhances credibility.

\paragraph{Limits of Design-based Solutions} Some threats cannot be fully addressed through design. Unobserved confounders that vary over time within units, measurement error in the treatment variable itself, and model misspecification remain concerns even in well-designed studies. Design-based reasoning reduces reliance on modelling assumptions but does not eliminate the need for judgement about the plausibility of identifying assumptions. Sensitivity analysis (Chapter~\ref{ch:design-diagnostics}) quantifies how conclusions would change under different assumptions about the magnitude of remaining threats.
