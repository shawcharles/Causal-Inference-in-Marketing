\section{Experimental and Quasi-Experimental Regimes}
\label{sec:exp-quasi-regimes}

The question is not ``What model fits the data?'' but ``How was treatment assigned?'' Design-based inference anchors on the assignment mechanism, not on functional form assumptions. Angrist and Pischke's credibility revolution puts this transparency at the centre of causal inference \citet{angrist2010credibility}, and quasi-experimental methods have become central to marketing research \citet{goldfarb2022conducting}. These designs exploit natural variation in treatment assignment over time and across units, using panel data to construct credible counterfactuals. Parallel trends, unconfoundedness, and exclusion restrictions are not testable in the strict sense---we cannot verify that parallel trends would hold in the post-treatment period---but pre-treatment data can provide supportive or falsifying evidence, and transparent design choices and rigorous diagnostics can make these assumptions more or less credible (Chapter~\ref{ch:design-diagnostics}).

The key is to articulate the assignment mechanism clearly and to assess whether identification assumptions are plausible given the institutional context. Marketing applications often involve staggered adoption (loyalty programmes, product launches), geographic variation (advertising campaigns, pricing experiments), and policy changes (platform algorithm updates, privacy regulations). Each setting demands careful attention to spillovers, anticipation, and measurement alignment between platform metrics and econometric estimands.

When we know how treatment was assigned---whether by randomisation, by administrative rules, by staggered rollout schedules, or by targeting criteria---we can articulate the conditions under which observed differences in outcomes have a causal interpretation without needing to commit to strong parametric assumptions about functional forms or error distributions.

Marketing panels offer rich data but messy assignment. The opportunities are that repeated observations create multiple sources of identifying variation, and natural variation in timing or intensity often approximates quasi-experimental designs. The constraints are that full randomisation is rare, treatment effects unfold dynamically, and spillovers violate the Stable Unit Treatment Value Assumption (SUTVA).

This chapter develops a design-based framework for causal inference in marketing panels, showing how assignment mechanisms map to estimands and estimators.

We distinguish between two broad regimes. Formally, let $\mathbf{W}$ be the $N \times T$ matrix of treatment assignments with entries $W_{it}$, and let $\mathbf{Y}(\mathbf{w})$ denote the matrix of potential outcomes for any assignment plan $\mathbf{w}$. The assignment mechanism is the conditional probability of observing assignment $\mathbf{W}$ given the potential outcomes and covariates $\mathbf{X}$:
\[ \Pr(\mathbf{W} \mid \mathbf{Y}(\cdot), \mathbf{X}). \]
By \emph{experimental} we mean designs where treatment is assigned by a known randomisation mechanism independent of potential outcomes, satisfying unconfoundedness:
\[ \Pr(\mathbf{W} \mid \mathbf{Y}(\cdot), \mathbf{X}) = \Pr(\mathbf{W} \mid \mathbf{X}). \]
By \emph{quasi-experimental} we mean designs where treatment is not randomised, but institutional rules, timing, or thresholds create as-if-random variation that can be exploited given credible assumptions.

Randomised (experimental) designs provide the cleanest path to causal inference but are scarce in marketing at scale. Geo-experiments and platform A/B tests represent the leading examples of randomised designs in marketing, and we devote considerable attention to their design and analysis. Quasi-experimental designs, where treatment is assigned by deterministic rules, strategic targeting, or staggered rollouts, dominate observational marketing data. When these assignments are driven by factors unrelated to the trajectory of untreated potential outcomes---or when we can condition on those factors---credible causal inference remains possible under explicit assumptions such as parallel trends or factor structures, which can be assessed through pre-treatment diagnostics even if they cannot be tested directly.

The notation and estimands introduced in Chapter~\ref{ch:frameworks} provide the foundation for this discussion. Recall that we observe outcomes $Y_{it}$ and treatments $W_{it}$ for units $i = 1, \ldots, N$ over periods $t = 1, \ldots, T$. The potential outcomes $Y_{it}(w)$ or $Y_{it}(\underline{w}^t)$ represent the outcomes that would be realised under different treatment assignments. Our target estimands --- the average treatment effect on the treated (ATT), cohort-time effects $\text{ATT}(g, t)$, event-time effects $\theta_k$ --- summarise causal contrasts between these potential outcomes.

The assignment mechanism determines which potential outcomes are observed and which remain counterfactual, and credible inference hinges on our ability to construct valid counterfactuals for the unobserved potential outcomes. Chapter~\ref{ch:frameworks} develops the formal framework; this chapter focuses on practical design questions: Who randomised what? What institutional features drive treatment assignment? Which spillovers are plausible? How should we plan diagnostics and inference?

Design-based reasoning contrasts with model-based adjustment, which specifies parametric models for outcomes as functions of treatments and covariates. Model-based approaches require strong functional form assumptions---linearity, additivity, correct specification of interactions---that are difficult to justify a priori and can lead to misleading inferences when violated. Design-based reasoning relaxes these requirements by anchoring inference in the assignment mechanism rather than in the outcome model.

This dichotomy is pedagogically useful but overstated in practice. Modern causal inference combines both: design-based reasoning justifies the identification strategy, while model-based adjustment (regression, propensity scores, outcome modelling) improves efficiency and addresses covariate imbalance. Regression remains a useful tool for estimation even in design-based settings, but the justification for the estimator flows from the assignment mechanism, not from the correctness of the regression specification.

Marketing panels typically fall into one of four design categories, each with distinct implications for identification and estimation. These categories are ideal types. Real applications often combine features, such as a geo-experiment with staggered rollout. Randomised cluster designs, exemplified by geo-experiments and platform A/B tests, assign treatment to groups of units through a randomisation protocol. Staggered adoption designs, common in phased rollouts of loyalty programmes or platform expansions, create variation in the timing of treatment adoption across units. Single treated unit designs arise naturally in case studies and flagship launches. Single treated period designs, where all units experience a common shock at a single point in time, apply to policy changes or regulatory interventions.

Each design maps to specific estimators developed in subsequent chapters. Randomised cluster designs enable difference-in-differences comparisons, though inference must account for clustering and potential spillovers (Chapter~\ref{ch:inference}). Staggered adoption designs call for modern heterogeneity-robust difference-in-differences estimators that avoid the negative weighting problems of traditional two-way fixed effects regressions (Chapters~\ref{ch:did}, \ref{ch:event}). Single treated unit designs motivate synthetic control methods that construct weighted combinations of control units to approximate the counterfactual trajectory of the treated unit (Chapters~\ref{ch:sc}, \ref{ch:generalized-sc}). Single treated period designs leverage event-study specifications that trace dynamic responses before and after the shock, testing for anticipation effects and estimating cumulative impacts (Chapter~\ref{ch:event}).

The modern panel data literature has clarified the conditions under which each of these designs yields credible causal estimates. Parallel trends assumptions, factor structures, conditional independence, and interference-aware models each correspond to particular features of the assignment mechanism and the data-generating process. This chapter provides practical guidance on designing marketing panel studies that align with these identification strategies, choosing appropriate estimands given the assignment mechanism, and planning diagnostics and inference \textit{ex ante} to ensure that the resulting estimates are both credible and policy-relevant.

We begin by classifying the major assignment mechanisms that define these regimes.
