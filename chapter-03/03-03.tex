
\section{Geo-Experiments and Clustered Designs}
\label{sec:geo-experiments-design}

Geo-experiments randomise treatment across geographic units---DMAs, stores, regions, cities---and have become the gold standard for measuring aggregate market-level causal effects in marketing. Major platforms now offer tools that handle randomisation and initial analysis, making geo-experiments accessible at scale. However, geo-experiments measure market-level effects rather than individual-level effects, require sufficient geographic variation in treatment, and may have limited power when the number of markets is small. They instantiate the block designs introduced in Section~\ref{sec:assignment-estimands} in a clustered, often fully experimental regime.

Designing a credible geo-experiment requires careful choices about cluster definition, stratification, treatment window length, and inference procedures. We provide practical guidance on these design decisions, grounded in the potential outcomes framework and the design-based principles articulated above.

\subsection{Cluster Definition and Stratification}

The first design choice is the definition of the cluster: the unit of randomisation. In geo-experiments, clusters are typically geographic markets---DMAs defined by Nielsen for television advertising, metropolitan areas, postal codes, or custom regions tailored to the firm's operating footprint. Formally, we partition the $N$ elementary units into $K$ clusters $\mathcal{C}_1, \dots, \mathcal{C}_K$. Treatment is assigned at the cluster level, so $W_{it} = \tilde{W}_{k,t}$ for all $i \in \mathcal{C}_k$.

The choice of cluster definition involves trade-offs. Larger clusters better internalise spillovers because consumers and competitors within a cluster interact more than consumers across clusters. Smaller clusters increase the number of clusters for a given budget, improving statistical power. The optimal cluster size depends on the strength of spillovers, the variance of outcomes within and between clusters, and the total number of units available for the experiment.

Stratification divides clusters into groups (strata) based on observable characteristics---market size, demographic composition, historical sales levels, competitive intensity---and randomises treatment within each stratum. Stratification ensures balance on the stratifying variables, reducing the variance of the treatment effect estimate and protecting against unlucky randomisations that assign treatment disproportionately to clusters with atypical characteristics. For example, if a retailer operates stores in both urban and rural markets, stratifying by urbanicity ensures that urban and rural stores are represented in both treatment and control groups in proportion to their prevalence in the population.

Over-stratification can reduce power. If strata are too fine, some strata may contain only one or two clusters, leaving little variation for within-stratum comparisons. A practical rule of thumb is to ensure at least four clusters per stratum (two treated, two control) to maintain adequate power.

Re-randomisation extends stratification by generating multiple candidate randomisations, checking balance on a set of covariates for each candidate, and selecting the randomisation that achieves the best balance according to a pre-specified criterion (for example, the smallest maximum standardised difference across covariates). Re-randomisation improves balance without biasing the estimate of the treatment effect, provided that the balance criterion is specified \textit{ex ante} and the final randomisation is selected using that criterion alone.

\textbf{Inference after re-randomisation.} Standard randomisation inference must be adjusted when re-randomisation is used. The reference distribution for the test statistic should include only those randomisations that would have passed the balance criterion, not all possible randomisations. Using standard inference after re-randomisation understates uncertainty because it ignores the selection step.

\subsection{Treatment Windows and Seasonality}

The treatment window is the period during which treatment is applied and outcomes are measured. Choosing an appropriate window length involves balancing statistical power (longer windows provide more data and more precise estimates) against the risk of confounding events (longer windows increase the chance that other shocks affect treated and control units differently). In marketing, seasonality is a pervasive confounder. Sales, advertising effectiveness, and competitive intensity vary systematically by week of year, month, and quarter due to holidays, weather, and consumer behaviour patterns.

Design-based solutions to seasonality include choosing treatment windows that span multiple seasons or that align treatment and control periods to the same seasonal phase. For example, if the goal is to estimate the effect of a holiday advertising campaign, the treatment window should cover the holiday season, and the control group should be observed during the same seasonal period (either in a previous year or in a concurrent, spatially separated control region). Alternatively, the analysis can include seasonal fixed effects or detrend outcomes using historical seasonal patterns. These model-based adjustments reintroduce functional form assumptions, but seasonal patterns are often well-understood and stable, making such adjustments relatively safe. The design-based vs. model-based distinction is a matter of degree, not a binary choice.

\subsection{Buffer Zones and Spillovers}

Geographic spillovers arise when treatment in one market affects outcomes in nearby markets. Customers may travel across market boundaries, media markets may overlap, or advertising may reach audiences outside the target geography. Buffer zones---untreated regions surrounding treated markets that are excluded from the analysis---mitigate spillovers by creating geographic separation between treatment and control units. The width of the buffer depends on the expected strength and spatial decay of spillovers. If spillovers dissipate quickly with distance, narrow buffers suffice. If spillovers are long-range, wide buffers are needed, reducing the effective sample size.

Buffer zones address spatial spillovers but not all spillover types. Network spillovers (for example, social media word-of-mouth or supply chain effects) and competitive spillovers (for example, national pricing responses) may not decay with geographic distance. When non-spatial spillovers are plausible, buffer zones provide limited protection, and explicit spillover models (Chapter~\ref{ch:spillovers}) are required.

When spillovers are central to the research question, the design should measure outcomes in buffer zones to estimate spillover effects directly rather than ignoring them. For example, if the goal is to estimate both the direct effect of an advertising campaign on sales in treated markets and the indirect effect on sales in neighbouring markets, the analysis compares outcomes in treated markets, neighbouring buffer markets, and distant control markets. This three-group design enables identification of direct and spillover effects jointly, provided that the buffer markets are far enough from distant controls that spillovers do not reach the control group.

The three-group design is demanding. It requires correct specification of the spatial structure of spillovers: which markets are "neighbours" and which are "distant." Misspecification of this structure can bias both direct and spillover effect estimates. When the spillover structure is uncertain, sensitivity analysis across different neighbourhood definitions is essential. Chapter~\ref{ch:spillovers} formalises these designs and develops estimators for direct and spillover effects.

\subsection{Inference with Few Clusters and Serial Correlation}

Geo-experiments often feature a modest number of clusters---dozens of DMAs, tens of regions, or even fewer custom markets---and outcomes measured over multiple periods within each cluster. This structure creates two inferential challenges. First, the small number of clusters means that standard cluster-robust standard errors, which rely on large-cluster asymptotics, may be unreliable. Second, outcomes are serially correlated within clusters over time because persistent unobservables, autocorrelated shocks, or dynamic feedback generate correlation across periods.

Randomisation inference provides an exact finite-sample solution that does not rely on asymptotic approximations. Under the sharp null hypothesis of no effect for any unit in any period, the observed treatment assignment is just one of many possible randomisations that could have been drawn, and recomputing the test statistic across these randomisations yields its null distribution. Comparing the observed statistic to this distribution gives an exact p-value that respects the randomisation protocol and automatically accounts for clustering and serial correlation without parametric assumptions.

The wild cluster bootstrap offers a complementary approach, resampling entire clusters with random signs on cluster-level residuals to preserve the within-cluster correlation structure. It is particularly effective when the number of clusters is small (fewer than 50) and when outcomes exhibit complex patterns of heteroskedasticity and serial correlation. Chapter~\ref{ch:inference} provides detailed algorithms and practical guidance for both randomisation inference and the wild cluster bootstrap in geo-experiments.

Geo-experiments randomise across units. An alternative design randomises within units over time.
