\section{Switchbacks and Platform Experiments}
\label{sec:switchbacks}

Switchback designs randomise treatment over time within units rather than across units. In a switchback, each unit alternates between treatment and control conditions according to a pre-specified schedule, so that the same unit serves as its own control across different time periods. Switchbacks are common in platform experiments where logistical or ethical concerns preclude withholding treatment from some users permanently, and where platforms prioritise rapid iteration and learning over long-term effect estimation. Note that platforms tend to use switchbacks in settings where they work well---short-lived effects, rapid iteration cycles---so the published literature may overrepresent success cases and underrepresent settings where carryover undermines the design. Within our taxonomy, switchbacks hold the unit fixed and randomise over time blocks, contrasting with geo-experiments (across units) and phased rollouts (staggered cohorts).

For example, a ride-sharing platform testing a new surge pricing algorithm might implement a switchback where each city alternates between the new algorithm (treatment) and the existing algorithm (control) on alternating weeks. This allows the platform to measure the effect while ensuring that no city is permanently denied the potential benefits of the new algorithm.

The simplest switchback alternates treatment and control in a fixed pattern: treatment in odd periods, control in even periods. More sophisticated designs use period blocking---stratifying by day of week, time of day, or other temporal characteristics---to ensure that treatment and control observations are balanced across temporal confounders. As with stratification in geo-experiments, over-blocking can reduce power: if blocks are too fine, few observations remain within each block for comparison. The key identifying assumption is that potential outcomes under control are stable over time, so that the control outcomes observed in even periods provide valid counterfactuals for the treated outcomes in odd periods. However, several threats can violate this assumption.

The natural estimand in a simple switchback is the average difference in outcomes between treated and control periods for the same unit (or a collection of similar units). Under no carryover, this estimates the instantaneous treatment effect. With carryover, the estimand becomes ambiguous: the observed difference conflates the immediate effect of current treatment with lingering effects of past treatment. In such cases, the analyst must decide \textit{ex ante} whether the target is the immediate effect, the cumulative effect over a treatment cycle, or the steady-state effect under continuous treatment. Chapter~\ref{ch:dynamics} develops methods for decomposing dynamic responses into short-run and cumulative components.

\subsection{Carryover, Anticipation, and Washout Periods}

The primary threat to switchback designs is carryover: treatment effects that persist into subsequent control periods. If an advertising campaign shown in period $t$ influences purchases in period $t+1$, then the control outcome in period $t+1$ is contaminated by the treatment effect from period $t$, biasing the switchback estimate downward. Formally, the potential outcome $Y_{it}$ depends not just on current treatment $w_{it}$ but on the treatment history $\underline{w}_{i}^t = (w_{i1}, \dots, w_{it})$. A generic no-carryover assumption---sometimes called \emph{temporal SUTVA} or \emph{no temporal interference}---posits $Y_{it}(\underline{w}_{i}^t) = Y_{it}(w_{it})$. Switchback designs with rapid alternation require a relaxed version, such as order-$L$ carryover:
\[ Y_{it}(\underline{w}_{i}^t) \approx Y_{it}(w_{it}, w_{i,t-1}, \dots, w_{i,t-L}). \]
Anticipation effects---where users respond to expected future treatments---create a symmetric problem, contaminating pre-treatment control periods with anticipatory responses.

Washout periods---gaps between treatment and control periods during which no data are collected---mitigate carryover by allowing effects to dissipate before the control period begins. The length of the washout depends on the expected half-life of the treatment effect. If advertising effects decay exponentially with a half-life of one week, a two-week washout ensures that less than 25 per cent of the initial effect remains: two half-lives reduce the effect to $(1/2)^2 = 0.25$ of its original magnitude. However, not all effects decay exponentially. Some effects have delayed peaks (for example, word-of-mouth that builds over time), non-monotonic decay (for example, habituation followed by recovery), or step-function decay (for example, effects that persist until a threshold is crossed). When the decay structure is uncertain, sensitivity analysis across different washout lengths is advisable. Chapter~\ref{ch:dynamics} develops methods for estimating the carryover structure, which informs washout length in future switchback experiments.

When carryover is inevitable and washouts are impractical, the analysis must model the carryover explicitly. Distributed lag specifications (Chapter~\ref{ch:dynamics}) include current and lagged treatments as regressors, estimating both the immediate effect and the lagged effects jointly. Event-study specifications (Chapter~\ref{ch:event}) trace the dynamic response, revealing the persistence of effects and enabling decomposition of the total effect into immediate and cumulative components.

\subsection{Learning Systems and Pacing}

Platform experiments often run concurrently with algorithmic learning systems that optimise targeting, bidding, or personalisation in real time. If the learning system adapts within the experimental window, treatment and control groups may diverge not just because of the treatment itself but because the learning system responds differently to treatment and control outcomes. This confounds the treatment effect with the learning effect. Design-based solutions include freezing the learning system during the experiment, running the experiment in a separate environment isolated from the learning system, or modelling the learning dynamics explicitly.

\textbf{External validity caveat.} Freezing the learning system during the experiment changes the estimand. The estimated effect is the treatment effect \emph{conditional on a frozen learning system}, which may differ from the effect in production where the learning system is active and adapts to the treatment. If the learning system would amplify or dampen the treatment effect in production, the experimental estimate may not generalise.

Pacing---the rate at which a budget or inventory is spent over time---introduces another complication. If a platform experiment randomises ad delivery but the campaign budget runs out early in the treatment period, then treated users in later periods receive no ads, diluting the treatment effect. Specifying pacing rules in advance and ensuring that budget and inventory are sufficient to cover the full treatment window prevent this issue. Alternatively, budget exhaustion can be treated as informative: if the treatment budget runs out early, that reveals something about treatment intensity and demand, which can be modelled as treatment effect heterogeneity rather than simply excluded. These complications are primarily design problems. The experiment should be embedded in a stable learning and pacing regime rather than relying solely on post-hoc modelling adjustments.

Switchback designs offer advantages when permanent treatment assignment is infeasible or when rapid learning is prioritised over long-run effect estimation. However, they require careful attention to carryover effects and temporal stability assumptions. When these assumptions are plausible and washout periods are feasible, switchbacks provide a powerful tool for causal inference in dynamic platform environments. When carryover is severe or temporal stability is suspect, alternative designs such as geo-experiments or phased rollouts may be more appropriate.

Switchbacks randomise treatment over time within units. A complementary approach staggers treatment adoption across units over time.
