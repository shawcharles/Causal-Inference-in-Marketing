\section{Matrix Completion Perspective}
\label{sec:factor-matrix}

The matrix completion perspective views the outcome matrix as partially observed, with missing or treated cells, and seeks to impute the unobserved entries by exploiting low-rank structure. This section presents nuclear-norm minimisation as a convex surrogate for rank minimisation, discusses missingness patterns and their implications for recovery, and addresses the bias-variance trade-off through regularisation. See Chapter~\ref{ch:advanced-matrix} for advanced matrix and tensor methods when panels are multi-way, non-stationary, or outlier-contaminated.

\subsection*{Formalisation and Rank Minimisation}

Matrix completion formalises the imputation problem as follows. The $N \times T$ outcome matrix $\mathbf{Y}$ is partially observed. We observe entries $Y_{it}$ for $(i, t) \in \Omega$, where $\Omega$ is the set of observed cells (untreated unit-period pairs). We seek to impute entries $Y_{it}$ for $(i, t) \notin \Omega$ (treated or missing cells).

If $\mathbf{Y}$ has low rank---the IFE model with rank $R$ implies that the noiseless component $\Lambda F'$ has rank $R$---then the unobserved entries can be recovered by solving:
\[
\min_{\mathbf{M}} \text{rank}(\mathbf{M}) \quad \text{subject to} \quad M_{it} = Y_{it} \text{ for all } (i, t) \in \Omega.
\]
This optimisation finds the lowest-rank matrix $\mathbf{M}$ that matches the observed entries exactly.

Figure~\ref{fig:factor-matrix} illustrates the matrix completion setup. The observed cells (control units and pre-treatment periods) are shaded, while the treated cells to be imputed are blank. The low-rank structure implies that the observed cells contain enough information to recover the missing entries.

\subsection*{Nuclear-Norm Minimisation}

The rank minimisation problem is computationally intractable (NP-hard), motivating convex relaxation. The nuclear norm of a matrix $\mathbf{M}$, defined as the sum of its singular values,
\[
\| \mathbf{M} \|_* = \sum_{k=1}^{\min(N,T)} \sigma_k(\mathbf{M}),
\]
serves as a convex surrogate for rank \citep{candes2009exact, candes2010power}. The regularised matrix completion estimator solves:
\[
\hat{\mathbf{M}} = \arg\min_{\mathbf{M}} \left( \frac{1}{2} \sum_{(i,t) \in \Omega} (Y_{it} - M_{it})^2 + \lambda \| \mathbf{M} \|_* \right),
\]
where $\lambda > 0$ is a regularisation parameter that controls the complexity (effective rank) of the solution. Large $\lambda$ encourages low-rank solutions; small $\lambda$ prioritises fitting the observed entries.

\subsection*{Soft-Impute Algorithm}

The nuclear-norm optimisation is solved using the Soft-Impute algorithm, which applies singular value thresholding (SVT) iteratively. Define the projection operators: $P_\Omega(\mathbf{Y})$ extracts the observed entries of $\mathbf{Y}$ (setting unobserved entries to zero), and $P_{\Omega^\perp}(\mathbf{M})$ extracts the unobserved entries of $\mathbf{M}$ (setting observed entries to zero). Box~\ref{box:soft-impute} describes the procedure.

\begin{quote}
\textbf{Box 8.2: Soft-Impute Algorithm}\label{box:soft-impute}

\textbf{1. Initialise:} Set $\mathbf{M}^{(0)} = \mathbf{0}$ or warm-start from a solution at a nearby $\lambda$.

\textbf{2. Fill Missing Entries:} Construct $\mathbf{Z}^{(k)} = P_\Omega(\mathbf{Y}) + P_{\Omega^\perp}(\mathbf{M}^{(k)})$, replacing observed entries with data and missing entries with current imputations.

\textbf{3. Apply SVT:} Compute the SVD $\mathbf{Z}^{(k)} = \mathbf{U} \mathbf{S} \mathbf{V}'$ and soft-threshold the singular values: $\mathbf{M}^{(k+1)} = \mathbf{U} \, \text{diag}\bigl((\sigma_j - \lambda)_+\bigr) \, \mathbf{V}'$.

\textbf{4. Check Convergence:} Iterate steps 2--3 until $\| \mathbf{M}^{(k+1)} - \mathbf{M}^{(k)} \|_F / \| \mathbf{M}^{(k)} \|_F < 10^{-5}$, typically 20--100 iterations.
\end{quote}

The algorithm is efficient because only the leading singular values are needed when the solution is low-rank. Warm-starting from a nearby $\lambda$ value speeds up computation when searching over a grid.

\subsection*{Regularisation and Bias-Variance Trade-Off}

The regularisation parameter $\lambda$ controls the bias-variance trade-off. Small $\lambda$ prioritises fitting observed entries (low bias, high variance); the imputed matrix may overfit idiosyncratic noise. Large $\lambda$ prioritises low rank (high bias, low variance); the imputed matrix is smooth but may underfit systematic structure. Cross-validation provides a data-driven choice: partition $\Omega$ into training (80\%) and validation (20\%) sets, solve for a grid of $\lambda$ values (e.g., $\lambda \in \{0.1, 0.5, 1, 2, 5, 10\} \times \sigma_1$, where $\sigma_1$ is the largest singular value), and select the $\lambda$ minimising validation error.

\subsection*{Connection to IFE}

Matrix completion with nuclear-norm regularisation is closely related to IFE estimation (Section~\ref{sec:factor-ife}). Both exploit low-rank structure to impute counterfactuals. The key differences are as follows. First, IFE requires specifying the rank $R$ explicitly, whereas matrix completion selects the effective rank implicitly through $\lambda$. Second, IFE uses hard-thresholding (keeping only the first $R$ singular values), while matrix completion uses soft-thresholding (shrinking all singular values by $\lambda$). Third, both methods benefit from two-way demeaning to remove additive fixed effects before estimation.

Use IFE when the rank is well-determined and explicit loadings are desired for interpretation. Use matrix completion when the rank is uncertain or the missingness pattern is complex.

\subsection*{Recovery Conditions}

The quality of matrix completion depends on the missingness pattern. \citet{candes2009exact} established that if the rank-$R$ matrix has incoherent singular vectors and the observed cells are sampled uniformly at random, then the matrix can be recovered exactly with high probability if the number of observed cells satisfies:
\[
|\Omega| \geq C \cdot R (N + T) \log^2(N + T),
\]
where $C$ is a constant. This means that recovery requires observing at least $O(R)$ entries per row and per column on average.

\paragraph{Random missingness.} If each cell is observed independently with probability $p$, recovery is feasible when $p \geq C R \log(NT) / \min(N,T)$. For a rank-5 matrix with $N = T = 100$, this requires observing roughly 50\% of entries.

\paragraph{Design-driven missingness.} In causal inference, missingness is not random but determined by the treatment assignment. Treated cells are missing by definition. Recovery is feasible if the observed cells span both the row space (loadings) and the column space (factors). This requires that control units are similar to treated units (their loadings span the same subspace) and that pre-treatment periods are similar to post-treatment periods (factors are stable).

\subsection*{Implications for Causal Inference}

The quality of counterfactual imputation depends on two conditions. First, the treated units' loadings must lie in (or near) the span of the control units' loadings---analogous to the convex hull requirement in synthetic control (Chapter~\ref{ch:sc}), though matrix completion does not require strict convexity. Second, the post-treatment factors must be similar to pre-treatment factors (no structural breaks); if the factor structure changes at the treatment date, pre-treatment data do not inform post-treatment imputation.

The connection to synthetic control is instructive. SC constructs the counterfactual as $\hat{Y}_{it}(0) = \sum_j w_j Y_{jt}$, assuming the treated unit's loading is a convex combination of control loadings. Matrix completion is more flexible (no convexity constraint) but requires stronger rank assumptions.

\subsection*{Treatment Effect Estimation}

With the imputed matrix $\hat{\mathbf{M}}$, treatment effects are computed as:
\[
\hat{\tau}_{it} = Y_{it} - \hat{Y}_{it}(0) \quad \text{for } (i,t) \notin \Omega,
\]
where $\hat{Y}_{it}(0) = \hat{\alpha}_i + \hat{\delta}_t + \hat{M}_{it}$ and $\hat{M}_{it}$ is the imputed demeaned component. For the overall ATT, aggregate across treated cells. Event-time and cohort-specific effects are computed by aggregating over appropriate subsets, as in Chapter~\ref{ch:generalized-sc}.

\subsection*{Practical Implementation}

Practical implementation requires several choices. Pre-process by constructing the outcome matrix with units as rows and periods as columns, flagging treated cells, and applying two-way demeaning to remove additive fixed effects. Use Soft-Impute (Box~\ref{box:soft-impute}) or alternating least squares. Select $\lambda$ via cross-validation as described above. Post-process by adding back the row and column means to the imputed matrix. Quantify uncertainty using bootstrap or cross-validation-based intervals (Section~\ref{sec:factor-inference}).

\paragraph{Software.} In R, the \texttt{softImpute} package provides general matrix completion with nuclear-norm regularisation. The \texttt{MCPanel} package implements matrix completion for causal panel analysis, including cross-validation and treatment effect estimation. In Python, \texttt{fancyimpute} provides similar functionality.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.85\textwidth]{images/fig_factor_matrix.pdf}
\caption{Matrix Completion Setup for Causal Panel Analysis}
\label{fig:factor-matrix}
\small
\textit{Note}: The $N \times T$ outcome matrix is partially observed. Shaded cells (control units across all periods; treated units in pre-treatment periods) are observed. Blank cells (treated units in post-treatment periods) are imputed using the low-rank structure estimated from observed cells. The nuclear-norm penalty encourages low-rank solutions that generalise from observed to missing cells.
\end{figure}
