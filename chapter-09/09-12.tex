\section{Conclusion and Future Directions}
\label{sec:adv-matrix-conclusion}

This chapter has extended the matrix completion framework from Chapter~\ref{ch:factor} to cover advanced methods for complex panel structures. Table~\ref{tab:advanced-matrix-comparison} summarises the methods.

\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Chapter 9: Key Takeaways]
\begin{enumerate}
\item \textbf{Tensor completion} (Section~\ref{sec:tensor-completion}) preserves multi-way structure and improves imputation accuracy when data have natural multi-way organisation.
\item \textbf{Robust methods} (Section~\ref{sec:robust-matrix}) separate outliers from the low-rank structure and reduce bias when data quality issues are present.
\item \textbf{Covariate-assisted methods} (Section~\ref{sec:matrix-side-info}) incorporate side information and reveal treatment effect heterogeneity.
\item \textbf{Time-varying rank models} (Section~\ref{sec:time-varying-rank}) adapt to non-stationary environments and structural breaks.
\item \textbf{Bayesian methods} (Section~\ref{sec:bayesian-matrix}) provide principled uncertainty quantification through posterior distributions.
\item \textbf{Efficient algorithms} (Section~\ref{sec:computation}) scale to large panels with millions of cells.
\end{enumerate}
\end{tcolorbox}

\subsection*{When to Use Advanced Methods}

For detailed method selection guidance, see Section~\ref{sec:adv-matrix-connections} and Section~\ref{sec:adv-matrix-workflow}.

Use tensor completion when data are naturally multi-way and the multi-way structure is informative. Use robust methods when outliers are common ($>$5\% of cells) and large ($>$3 standard deviations). Use covariate-assisted methods when rich side information is available and treatment effects are heterogeneous. Use time-varying rank models when structural breaks are present in the factor structure. Use Bayesian methods when uncertainty quantification is critical and computational resources permit.

\subsection*{Open Research Questions}

Several research questions remain open:
\begin{enumerate}
\item How can we extend tensor completion to handle time-varying rank in multi-way panels?
\item How can we combine robust methods with Bayesian uncertainty quantification?
\item How can we scale Bayesian methods to panels with billions of cells?
\item How can we incorporate network structure (spatial correlation, competitive relationships) into tensor completion?
\item How can we handle non-convex regularisers (e.g., rank constraints) efficiently?
\end{enumerate}

\subsection*{Emerging Methods}

\paragraph{Deep learning for matrix completion.} Neural networks can learn nonlinear low-rank structures that linear methods miss. Autoencoders provide a natural framework for matrix completion. Generative adversarial networks (GANs) can impute counterfactuals by learning the distribution of untreated outcomes. However, deep learning methods require large datasets and careful tuning. They are most promising for very large panels (millions of cells) where traditional methods are computationally infeasible.

\paragraph{Integration with causal machine learning.} Matrix completion can be combined with double machine learning to estimate heterogeneous treatment effects (Chapters~\ref{ch:ml-nuisance} and~\ref{ch:high-dim}). Lasso and group-lasso can select relevant covariates in covariate-assisted matrix completion. Cross-fitting avoids overfitting in high-dimensional settings. These hybrid methods combine the strengths of matrix completion (exploiting low-rank structure) and machine learning (handling high-dimensional covariates).

\subsection*{Summary}

The practical workflow and software recommendations in Section~\ref{sec:adv-matrix-workflow} provide a starting point for implementation. The four case studies in Section~\ref{sec:adv-matrix-applications} demonstrate value in real marketing applications. By carefully selecting the appropriate method and conducting rigorous diagnostics (Section~\ref{sec:adv-matrix-diagnostics}), practitioners can achieve credible causal estimates in challenging settings.

\begin{table}[htbp]
\begin{tighttable}
\centering
\caption{Comparison of Advanced Matrix Methods}
\label{tab:advanced-matrix-comparison}
\begin{tabularx}{\textwidth}{Y Y Y Y}
\toprule
\textbf{Method} & \textbf{Key Feature} & \textbf{When to Use} & \textbf{Computational Cost} \\
\midrule
Tensor Completion & Preserves multi-way structure & Multi-way data with informative structure & High (scales with modes) \\
\addlinespace
Robust MC & Separates outliers from low-rank & Outliers present ($>$5\% of cells) & Medium (ADMM) \\
\addlinespace
Covariate-Assisted & Incorporates side information & Rich covariates available & Low \\
\addlinespace
Time-Varying Rank & Adapts to structural breaks & Non-stationary factor structure & Medium \\
\addlinespace
Bayesian & Uncertainty quantification & Credible intervals needed & High (MCMC) \\
\addlinespace
Soft-Impute & Efficient for sparse data & Large sparse panels & Low \\
\bottomrule
\end{tabularx}
\end{tighttable}
\end{table}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{images/fig_tensor_schematic.pdf}
\caption{Tensor Completion Schematic for Three-Way Panels}
\label{fig:tensor-schematic}
\small
\textit{Note}: Panel A shows a three-way tensor (products $\times$ stores $\times$ time) with observed cells (blue) and treated/missing cells (red). Panel B illustrates the CP decomposition into product loadings, store loadings, and time factors. Panel C shows the counterfactual imputation workflow.
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\textwidth]{images/fig_robust_decomposition.pdf}
\caption{Robust Matrix Completion: Separating Low-Rank Structure from Outliers}
\label{fig:robust-decomposition}
\small
\textit{Note}: The decomposition $\mathbf{Y} = \mathbf{L} + \mathbf{S} + \mathbf{E}$ separates the observed matrix into a low-rank component (systematic structure), a sparse component (outliers), and noise. Panel A shows the observed matrix with outliers marked. Panel B shows the estimated low-rank component capturing trends and factor structure. Panel C shows the sparse component isolating outliers.
\end{figure}
