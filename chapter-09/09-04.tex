\section{Matrix Completion with Side Information}
\label{sec:matrix-side-info}

\subsection*{Incorporating Covariates}

Marketing panels often include rich side information. Store characteristics (size, location, demographics) vary across units. Time-varying covariates (weather, holidays, macroeconomic conditions) vary across periods. Network structure (geographic proximity, competitive relationships) links units. This side information provides additional signal that can improve counterfactual imputation.

\begin{definition}[Covariate-Assisted Matrix Completion Model]
The untreated potential outcomes follow:
\[
Y_{it}(0) = \underbrace{X_{it}^\top \beta}_{\text{covariate effect}} + \underbrace{\lambda_i^\top f_t}_{\text{low-rank component}} + \varepsilon_{it},\footnote{In this chapter, we denote the untreated potential outcome as $Y_{it}(0)$. Unlike the staggered adoption setting in Chapter 10 where $Y_{it}(\infty)$ is used, matrix completion methods allow for arbitrary treatment patterns, making the binary $Y_{it}(0)$ notation standard in this literature (e.g., Athey et al., 2021).}
\]
where:
\begin{enumerate}[(i)]
    \item $X_{it} \in \mathbb{R}^p$ is a vector of observed covariates for unit $i$ at time $t$;
    \item $\beta \in \mathbb{R}^p$ is the coefficient vector;
    \item $\lambda_i \in \mathbb{R}^R$ are unit-specific factor loadings;
    \item $f_t \in \mathbb{R}^R$ are time-specific factors;
    \item $\varepsilon_{it}$ are idiosyncratic errors with $\mathbb{E}[\varepsilon_{it} | X_{it}, \lambda_i, f_t] = 0$.
\end{enumerate}
\end{definition}

The covariates capture systematic variation explained by observed characteristics. The low-rank component captures residual co-movement after controlling for covariates.

\paragraph{Types of covariates.} Covariates fall into three categories. Unit-specific covariates are time-invariant characteristics such as store size, location (urban or rural), demographics (median income, population density), and format (supermarket or convenience), entering the model as $X_i' \beta$. Time-varying covariates include weather (temperature, precipitation), holidays, and macroeconomic conditions (unemployment, consumer confidence), entering as $X_{it}' \beta$. Network structure, capturing geographic proximity and competitive relationships, is handled by graph-regularised methods described below.

\subsection*{Graph-Regularised Matrix Completion}

Network structure links units through geographic proximity or competitive relationships. Graph-regularised matrix completion penalises differences between connected units.

\begin{definition}[Graph Laplacian]
Let $\mathcal{G} = ([N], \mathcal{E})$ be an undirected graph on units with adjacency matrix $\mathbf{A}$. The graph Laplacian is $\mathbf{L}_G = \mathbf{D} - \mathbf{A}$, where $\mathbf{D}$ is the degree matrix with $D_{ii} = \sum_j A_{ij}$.

\textbf{Intuition:} The quadratic form $\mathbf{x}^\top \mathbf{L}_G \mathbf{x} = \sum_{(i,j) \in \mathcal{E}} (x_i - x_j)^2$ measures the total squared difference between connected nodes. Penalising this term encourages similar values for connected units.
\end{definition}

\begin{definition}[Graph-Regularised Estimator]
The graph-regularised matrix completion estimator solves:
\[
\min_{\mathbf{M}} \sum_{(i,t) \in \Omega} (Y_{it} - M_{it})^2 + \lambda \|\mathbf{M}\|_* + \gamma \, \text{tr}(\mathbf{M} \mathbf{L}_G \mathbf{M}^\top),
\]
where $\lambda > 0$ controls low-rank regularisation (as in Section~\ref{sec:factor-tuning}), and $\gamma > 0$ controls graph smoothness. The trace term expands to:
\[
\text{tr}(\mathbf{M} \mathbf{L}_G \mathbf{M}^\top) = \sum_{(i,j) \in \mathcal{E}} \|\mathbf{M}_{i\cdot} - \mathbf{M}_{j\cdot}\|_2^2.
\]
\end{definition}

\subsection*{Estimation: Two-Step Procedure}

The following algorithm provides a simple and computationally efficient approach.

\begin{tcolorbox}[colback=green!5!white,colframe=green!50!black,title=Algorithm: Two-Step Covariate-Assisted Matrix Completion]
\textbf{Input:} Outcomes $\{Y_{it}\}$, covariates $\{X_{it}\}$, untreated cells $\Omega$, treated cells $\mathcal{T}$.

\paragraph{Step 1: Estimate covariate effect:}
\begin{enumerate}
\item Regress $Y_{it}$ on $X_{it}$ using only untreated cells $(i,t) \in \Omega$:
\[
\hat{\beta} = \arg\min_{\beta} \sum_{(i,t) \in \Omega} (Y_{it} - X_{it}^\top \beta)^2.
\]
\item Compute residuals: $\tilde{Y}_{it} = Y_{it} - X_{it}^\top \hat{\beta}$ for all $(i,t) \in \Omega$.
\end{enumerate}

\paragraph{Step 2: Matrix completion on residuals:}
\begin{enumerate}
\item Apply nuclear-norm regularised matrix completion to residuals $\{\tilde{Y}_{it}\}_{(i,t) \in \Omega}$:
\[
\hat{\mathbf{L}} = \arg\min_{\mathbf{L}} \sum_{(i,t) \in \Omega} (\tilde{Y}_{it} - L_{it})^2 + \lambda \|\mathbf{L}\|_*.
\]
\item Select $\lambda$ by cross-validation (Section~\ref{sec:factor-tuning}).
\end{enumerate}

\paragraph{Step 3: Impute counterfactuals:}
\begin{enumerate}
\item For each treated cell $(i,t) \in \mathcal{T}$, impute:
\[
\hat{Y}_{it}(0) = X_{it}^\top \hat{\beta} + \hat{L}_{it}.
\]
\item Compute treatment effects: $\hat{\tau}_{it} = Y_{it} - \hat{Y}_{it}(0)$.
\end{enumerate}

\paragraph{Output:} Counterfactuals $\{\hat{Y}_{it}(0)\}_{(i,t) \in \mathcal{T}}$ and treatment effects $\{\hat{\tau}_{it}\}$.
\end{tcolorbox}

\paragraph{Two-step vs joint estimation:} Alternatively, estimate $\beta$ and $\mathbf{L}$ jointly via the nuclear-norm-penalised objective (see Chapter~\ref{ch:factor}). Joint estimation can improve efficiency but increases computational cost. The two-step approach is adequate when covariates explain most variation (high $R^2$ in Step 1). Joint estimation may improve efficiency when the low-rank component is important (low $R^2$).

\subsection*{Identification with Covariates}

Covariates improve identification when correlated with outcomes but not with treatment assignment (conditional on the low-rank structure).

\begin{assumption}[Covariate Exogeneity]\label{ass:covariate-exogeneity}
The covariates $X_{it}$ satisfy:
\begin{enumerate}[(i)]
    \item \textbf{Strict exogeneity:} $\mathbb{E}[\varepsilon_{it} | X_{i1}, \ldots, X_{iT}, \lambda_i, f_1, \ldots, f_T] = 0$ for all $i, t$;
    \item \textbf{Rank condition:} $\mathbb{E}[X_{it} X_{it}^\top]$ is positive definite;
    \item \textbf{Treatment independence:} $\mathbb{E}[X_{it} | D_{it} = 1, \lambda_i, f_t] = \mathbb{E}[X_{it} | D_{it} = 0, \lambda_i, f_t]$.
\end{enumerate}
\end{assumption}

\begin{proposition}[Efficiency Gain from Covariates]\label{prop:covariate-gain}
Under Assumption~\ref{ass:covariate-exogeneity}, the covariate-assisted estimator achieves:
\[
\text{MSE}(\hat{\tau}_{\text{ATT}}^{\text{cov}}) = \text{MSE}(\hat{\tau}_{\text{ATT}}^{\text{std}}) \cdot (1 - R^2_X),
\]
where $R^2_X = \text{Var}(X_{it}^\top \beta) / \text{Var}(Y_{it}(0))$ is the fraction of outcome variance explained by covariates.
\end{proposition}

\begin{tcolorbox}[colback=orange!5!white,colframe=orange!75!black,title=Caution: Endogenous Covariates]
If covariates are correlated with treatment, they may introduce confounding:
\begin{itemize}
\item If treatment is more likely in large stores, and store size is a covariate, the covariate effect may absorb part of the treatment effect (downward bias).
\item Including potentially endogenous covariates (prices, advertising) requires careful identification.
\end{itemize}
\textbf{Guidance:} Include covariates known to affect outcomes (weather, holidays, demographics). Exclude covariates that may be affected by treatment or that are endogenous.
\end{tcolorbox}

\subsection*{Comparison: Standard vs Covariate-Assisted Matrix Completion}

Table~\ref{tab:covariate-comparison} compares standard and covariate-assisted approaches.

\begin{table}[htbp]
\begin{tighttable}
\centering
\caption{Standard vs Covariate-Assisted Matrix Completion}
\label{tab:covariate-comparison}
\begin{tabularx}{\textwidth}{Y Y Y}
\toprule
\textbf{Aspect} & \textbf{Standard MC} & \textbf{Covariate-Assisted MC} \\
\midrule
Model & $Y = L + E$ & $Y = X\beta + L + E$ \\
\addlinespace
Side information & Ignored & Incorporated \\
\addlinespace
Efficiency & Baseline & Improved by $(1 - R^2_X)$ \\
\addlinespace
When to use & No relevant covariates & Rich unit/time covariates \\
\addlinespace
Heterogeneity & Implicit in loadings & Explicit via covariate interactions \\
\addlinespace
Computation & Single-step MC & Two-step (regression + MC) \\
\addlinespace
Risk & None & Endogenous covariate bias \\
\bottomrule
\end{tabularx}
\end{tighttable}
\end{table}

\subsection*{Software Implementation}

Several software packages support covariate-assisted matrix completion. In Python, \texttt{fancyimpute} provides matrix completion with support for covariates via feature matrices, \texttt{surprise} offers a recommender system library with side information support for user and item features, and custom implementations using \texttt{scipy.optimize} can handle the two-step procedure. In R, \texttt{softImpute} provides nuclear-norm regularised matrix completion that can be combined with \texttt{lm()} for the two-step procedure, \texttt{cmfrec} implements collective matrix factorisation with side information, and \texttt{recosystem} offers matrix factorisation for recommender systems with features. For graph-regularised matrix completion, \texttt{pygsp} in Python provides graph signal processing tools that can be combined with matrix completion for graph-regularised estimation, while \texttt{igraph} combined with custom solvers allows building adjacency matrices and implementing ADMM.

\subsection*{Application: Store Characteristics and Demand}

Consider a retailer with 200 stores observing weekly sales of 100 products over 52 weeks. Store characteristics include size (square footage), location (urban/rural), and demographics (median income). A promotional campaign is applied to 20 products in 40 stores for 4 weeks.

\paragraph{Standard MC.} Ignores store characteristics. Estimates a low-rank structure capturing co-movement across stores and products. Does not account for systematic differences due to size and demographics.

\paragraph{Covariate-assisted MC.}
\begin{enumerate}
\item \textbf{Step 1:} Regress sales on store size, location, and demographics using untreated cells. First-step $R^2 = 0.60$ (covariates explain 60\% of variation).
\item \textbf{Step 2:} Apply matrix completion to residuals. The low-rank component captures residual co-movement.
\item \textbf{Imputation:} Combine covariate effect and low-rank effect for counterfactuals.
\end{enumerate}

\paragraph{Results.} The covariate-assisted estimator yields an ATT of 18\% (95\% CI: 14--22\%), compared to 15\% (95\% CI: 10--20\%) from standard matrix completion. The difference arises because the covariate-assisted approach accounts for store heterogeneity that standard methods ignore.

\paragraph{Heterogeneity by store type.} Treatment effects vary substantially by store size: large stores see a 25\% increase (95\% CI: 19--31\%), medium stores 15\% (95\% CI: 10--20\%), and small stores 10\% (95\% CI: 5--15\%). This heterogeneity is masked by standard matrix completion, which produces a single average effect.

\paragraph{Diagnostics.} Out-of-sample $R^2$: covariate-assisted MC achieves 0.75 vs 0.65 for standard MC. The 10 percentage point improvement indicates substantial value from covariates. Inference uses bootstrap methods (Section~\ref{sec:bayesian-matrix}).
