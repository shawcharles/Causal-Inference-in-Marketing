\section{The Two Cultures of Marketing Analytics}
\label{sec:two-cultures}

In an influential paper, statistician Leo Breiman distinguished two cultures in statistical modelling \citep{breiman2001statistical}. The first, data modelling, posits explicit probabilistic models and estimates parameters under strong functional form assumptions. The second, algorithmic modelling, treats the underlying mechanism as unknown and focuses on predictive accuracy via flexible, data-adaptive algorithms. Breiman argued that statistics had overemphasised the first culture while neglecting the second, missing opportunities to leverage machine learning for prediction.

This dichotomy, while influential, has been criticised as too stark. Good statistical practice has always included elements of both cultures. Data modellers routinely validate predictive performance through cross-validation. Algorithmic modellers increasingly seek interpretability via SHAP values, partial dependence plots, and related tools. The real tension is not between prediction and understanding, but between rigid parametric assumptions and flexible, data-driven methods. Importantly, neither culture as originally conceived addresses the central challenge of marketing analytics: moving from prediction to causal inference.

Modern marketing analytics reflect this tension. The rise of digital platforms has fuelled predictive modelling---churn prediction, recommendation engines, bid optimisation---that excels at forecasting outcomes within the current system. But when organisations face strategic decisions---Should we launch this loyalty programme? Will a price increase raise long-term revenue?---prediction alone is insufficient. We need causal inference. Panel data methods, grounded in quasi-experimental design but augmented with machine learning, provide a path forward.

Modern panel data methods for causal inference represent a synthesis of both cultures. They deploy the flexibility of algorithmic modelling to handle high-dimensional confounders and estimate heterogeneous treatment effects. Yet they maintain the discipline of data modelling by making causal structure explicit---parallel trends, unconfoundedness, factor structure---and subjecting those assumptions to diagnostic tests. Double machine learning exemplifies this synthesis: machine learning algorithms estimate nuisance functions with minimal parametric assumptions, while design-based identification ensures the final causal estimate is valid. This approach prioritises causal validity over pure predictive accuracy, but achieves it using tools from both traditions.

Our modelling philosophy is pragmatic. We do not seek a single true model that perfectly captures the data-generating process---no such model exists, and even if it did, we could not verify we had found it. Instead, we adopt a workflow with four components. First, we articulate the identification assumptions---parallel trends, conditional independence, factor structure, no interference---required for a causal interpretation. Second, we choose an estimator implementing that identification strategy: difference-in-differences, synthetic control, interactive fixed effects, or a doubly robust machine learning method. Third, we subject the estimates to diagnostics---pre-trend tests, placebo checks, balance assessments, leave-one-out robustness, specification curves---to assess whether assumptions hold and whether results are sensitive to modelling choices. Fourth, we conduct sensitivity analysis to quantify how large an assumption violation would need to be to overturn our conclusions, acknowledging that assumptions are approximations rather than exact truths. This workflow embodies cultural synthesis: flexible tools deployed within a disciplined causal framework.
