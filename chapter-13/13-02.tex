\section{High-Dimensional Controls in Panels}
\label{sec:hd-panels}

High-dimensional panel models extend standard panel regression to accommodate many controls. This section formalises the model, discusses how to partial out fixed effects and latent factors, and clarifies the implications of dependence for selection and inference.

The baseline model extends standard panel regression to a high-dimensional setting:
\[
Y_{it} = \tau D_{it} + X_{it}' \beta + \alpha_i + \gamma_t + \varepsilon_{it},
\]
where $D_{it}$ is the treatment indicator, $\tau$ the treatment effect, $X_{it} \in \mathbb{R}^p$ the control vector, and $\alpha_i$, $\gamma_t$ the unit and time fixed effects. The model is high-dimensional when $p$ is large relative to the effective sample size, necessitating regularisation to estimate $\beta$ and thereby isolate $\tau$. The controls absorb variation correlated with treatment, reducing omitted-variable bias, while regularisation balances the bias from omitting important confounders against the variance from including irrelevant ones.

\subsection*{Fixed Effects and the Within-Transformation}

As established in Chapter~\ref{ch:frameworks}, unit fixed effects $\alpha_i$ absorb time-invariant heterogeneity (store location, brand equity, baseline loyalty) while time fixed effects $\gamma_t$ capture common shocks (seasonality, macroeconomic conditions). In high-dimensional settings, fixed effects reduce the effective covariate dimension: time-invariant controls are collinear with $\alpha_i$ and drop out, while time-varying controls enter only after the within-transformation.

A common approach is to partial out fixed effects via demeaning before applying regularisation. The transformed regression of $\tilde{Y}_{it}$ on $\tilde{D}_{it}$ and $\tilde{X}_{it}$ treats fixed effects as nuisance parameters rather than as part of the high-dimensional problem.

\subsection*{Factor-Augmented Models}

Factor-augmented models (Chapter~\ref{ch:factor}) extend this logic to heterogeneous exposure to common shocks. Writing $Y_{it} = \tau D_{it} + X_{it}'\beta + \lambda_i' f_t + \varepsilon_{it}$, where $\lambda_i$ are unit-specific loadings and $f_t$ are common factors, regularisation can be applied after residualising with respect to the estimated factor structure $\hat{\lambda}_i' \hat{f}_t$. This combines factor flexibility with covariate parsimony.

\subsection*{Dependence Structures in Panels}

Panel data exhibit dependence structures, such as serial correlation within units and cross-sectional correlation across units. This dependence affects regularisation in two ways. First, the choice of penalty parameter must account for it to avoid selecting too many or too few controls. Second, statistical inference after selection must be adjusted to avoid understating uncertainty.

\paragraph{Clustering by unit.} This is the most common approach to dependence in panels. Errors are assumed to be independent across units but arbitrarily correlated within units over time. This assumption is plausible when units are distinct entities (stores, DMAs, customers) that do not interact, and when correlation within units arises from persistent unobserved characteristics or serially dependent shocks.

Cluster-robust variance estimators aggregate within-unit residuals and account for serial dependence, providing valid standard errors without specifying the exact form of dependence. Penalty choice under clustering uses blocked cross-validation (Section~\ref{sec:hd-tuning}), partitioning units into folds and ensuring that all observations from each unit are either in the training set or the validation set, never split across folds.

Clustering by time is appropriate when dependence is primarily across units within periods (for example, due to common shocks or spatial spillovers) and when the number of periods is large relative to the number of units. Errors are assumed to be independent across periods but arbitrarily correlated across units within periods. Cluster-robust variance estimators aggregate within-period residuals. Penalty choice uses time-blocked cross-validation, partitioning periods into folds.

\paragraph{Two-way clustering.} This accounts for dependence in both dimensions, allowing errors to be correlated within units over time and across units within periods. Two-way cluster-robust variance estimators are computed by aggregating residuals along both dimensions and combining the covariance matrices.

Two-way clustering is conservative (producing wider confidence intervals) but robust to complex dependence structures. Penalty choice under two-way clustering is challenging because cross-validation must block both dimensions, reducing the effective sample size for validation and increasing computational cost.

\subsection*{Implications for Selection and Inference}

Regularisation and post-selection procedures must respect the dependence structure. Naïve lasso (ignoring dependence, treating observations as independent) under-penalises in the presence of dependence, selecting too many controls and overfitting. Naïve standard errors (computed under independence) underestimate uncertainty, producing confidence intervals that are too narrow and p-values that are too small.

The solution is to use dependence-adjusted penalties (for example, theoretical penalty formulas that scale with the cluster size or the strength of dependence, or blocked cross-validation that respects clustering) and cluster-robust inference (accounting for within-cluster correlation in variance estimates). Chapter~\ref{ch:inference} provides comprehensive coverage of cluster-robust methods and small-sample adjustments (wild cluster bootstrap) for few clusters.
