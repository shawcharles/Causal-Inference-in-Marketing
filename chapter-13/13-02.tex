\section{High-Dimensional Controls in Panels}
\label{sec:hd-panels}

High-dimensional panel models extend standard panel regression to accommodate many controls. This section formalises the model, discusses how to partial out fixed effects and latent factors, and clarifies the implications of dependence for selection and inference.

The baseline model extends a standard panel regression to accommodate a high-dimensional vector of controls, $X_{it} \in \mathbb{R}^p$:
\[
Y_{it} = \tau W_{it} + X_{it}' \beta + \alpha_i + \gamma_t + \varepsilon_{it}.
\]
Here, $\tau$ is the treatment effect, while $\alpha_i$ and $\gamma_t$ are unit and time fixed effects. The model is high-dimensional when $p$ is large relative to the sample size, requiring regularisation to estimate $\beta$ and thereby identify $\tau$.

The controls $X_{it}$ are included to absorb variation correlated with treatment, reducing omitted-variable bias. Regularisation provides a data-driven method for selecting the conditioning set when theory does not specify it, balancing the bias from omitting important confounders against the variance from including irrelevant ones.

\subsection*{Fixed Effects and the Within-Transformation}

Unit and time fixed effects $\alpha_i$ and $\gamma_t$ absorb time-invariant unit characteristics (for example, store location, brand equity, baseline customer loyalty) and common time-varying shocks (for example, seasonality, macroeconomic conditions, category trends) that affect all units simultaneously. Fixed effects are identified from within-unit variation over time and within-time variation across units, provided that treatment varies within units or within periods.

In high-dimensional settings, fixed effects reduce the effective dimension of controls by removing variation that is absorbed by $\alpha_i$ and $\gamma_t$. For example, if demographic variables are time-invariant ($X_{it} = X_i$ for all $t$), they are collinear with unit fixed effects and cannot be included in the regression. If controls are time-varying ($X_{it}$ varies within unit $i$ over time), they enter the regression after unit and time effects are partialled out, reducing the residual covariate dimension and mitigating multicollinearity.

A common approach is to partial out the fixed effects via the within-transformation before applying regularisation. This involves demeaning the data for each unit and time period. Regularisation is then applied to a regression of the transformed outcome $\tilde{Y}_{it}$ on the transformed treatment $\tilde{W}_{it}$ and controls $\tilde{X}_{it}$. This two-step procedure is simple and transparent, but it treats the fixed effects as nuisance parameters to be removed rather than as part of the high-dimensional estimation problem.

\subsection*{Factor-Augmented Models}

Partialling out latent factors, as in Chapter~\ref{ch:factor}, extends this logic to accommodate heterogeneous exposure to common shocks. The factor-augmented model is:
\[
Y_{it} = \tau W_{it} + X_{it}' \beta + \lambda_i' f_t + \varepsilon_{it}.
\]
Regularisation can be applied after residualising the data with respect to the estimated factor structure, $\hat{\lambda}_i' \hat{f}_t$. This combines the flexibility of factor models with the parsimony of regularisation.

\subsection*{Dependence Structures in Panels}

Panel data exhibit dependence structures, such as serial correlation within units and cross-sectional correlation across units. This dependence affects regularisation in two ways. First, the choice of penalty parameter must account for it to avoid selecting too many or too few controls. Second, statistical inference after selection must be adjusted to avoid understating uncertainty.

\paragraph{Clustering by unit.} This is the most common approach to dependence in panels. Errors are assumed to be independent across units but arbitrarily correlated within units over time. This assumption is plausible when units are distinct entities (stores, DMAs, customers) that do not interact, and when correlation within units arises from persistent unobserved characteristics or serially dependent shocks.

Cluster-robust variance estimators aggregate within-unit residuals and account for serial dependence, providing valid standard errors without specifying the exact form of dependence. Penalty choice under clustering uses blocked cross-validation (Section~\ref{sec:hd-tuning}), partitioning units into folds and ensuring that all observations from each unit are either in the training set or the validation set, never split across folds.

Clustering by time is appropriate when dependence is primarily across units within periods (for example, due to common shocks or spatial spillovers) and when the number of periods is large relative to the number of units. Errors are assumed to be independent across periods but arbitrarily correlated across units within periods. Cluster-robust variance estimators aggregate within-period residuals. Penalty choice uses time-blocked cross-validation, partitioning periods into folds.

\paragraph{Two-way clustering.} This accounts for dependence in both dimensions, allowing errors to be correlated within units over time and across units within periods. Two-way cluster-robust variance estimators are computed by aggregating residuals along both dimensions and combining the covariance matrices.

Two-way clustering is conservative (producing wider confidence intervals) but robust to complex dependence structures. Penalty choice under two-way clustering is challenging because cross-validation must block both dimensions, reducing the effective sample size for validation and increasing computational cost.

\subsection*{Implications for Selection and Inference}

Regularisation and post-selection procedures must respect the dependence structure. Naïve lasso (ignoring dependence, treating observations as independent) under-penalises in the presence of dependence, selecting too many controls and overfitting. Naïve standard errors (computed under independence) underestimate uncertainty, producing confidence intervals that are too narrow and p-values that are too small.

The solution is to use dependence-adjusted penalties (for example, theoretical penalty formulas that scale with the cluster size or the strength of dependence, or blocked cross-validation that respects clustering) and cluster-robust inference (accounting for within-cluster correlation in variance estimates). Chapter~\ref{ch:inference} provides comprehensive coverage of cluster-robust methods and small-sample adjustments (wild cluster bootstrap) for few clusters.
