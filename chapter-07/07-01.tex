\section{Motivation and Setup}
\label{sec:hybrid-motivation}

Credibility and efficiency pull in opposite directions. Design-based methods earn trust by avoiding functional-form assumptions, but they pay a price. When the treated unit lies outside the donor pool's convex hull (Section~\ref{sec:sc-motivation}), weighting alone cannot close the gap. The fit deteriorates. Bias creeps in. Pure synthetic control, for all its transparency, sometimes fails.

This chapter confronts that failure and offers a resolution. Hybrid methods blend design-based weighting with outcome modelling to gain efficiency without abandoning the principles that made synthetic control credible. The question is whether the compromise works---and when it does not.

Consider what happens when a retailer pilots a loyalty programme in five flagship stores. The pilot runs for two years. The retailer has perhaps twenty control stores, but none resembles the flagships. These stores anchor high-traffic urban centres, draw different customer segments, and generate revenue patterns that no convex combination of suburban or regional stores can replicate. Standard synthetic control produces weights, but the pre-treatment fit is poor. The gap between the synthetic and actual trajectories signals trouble. Any post-treatment estimate carries the residual bias forward.

Hybrid methods attack this problem from three directions. \index{augmented synthetic control}Augmented synthetic control pairs the weighting estimator with a regression adjustment that corrects for the residual imbalance. If the weights produce a synthetic control that undershoots the treated unit's pre-treatment revenue by 5\%, the regression model estimates that gap and subtracts it from the treatment effect. Regularised synthetic control takes a different path: it shrinks the weights toward simplicity, trading a small increase in bias for a larger reduction in variance. Synthetic difference-in-differences introduces time weights alongside unit weights, aligning both the cross-sectional and temporal dimensions before computing contrasts.

Each approach relaxes a constraint that pure synthetic control imposes. You gain flexibility. But flexibility has costs. Adding a regression adjustment introduces model dependence---the estimate now depends on the specification of that adjustment model. Regularisation requires tuning parameters that practitioners must choose, often without clear guidance. Time weights assume a common trend structure that may not hold. Hybrid methods are not strictly superior to their predecessors. They occupy a different point on the bias-variance frontier, and the right choice depends on the data structure you face.

The factor model foundation developed in Chapter~\ref{ch:sc} remains central. Recall that untreated potential outcomes decompose into unit-specific loadings and time-varying factors. When the treated unit's loadings lie outside the convex hull of donor loadings, pure synthetic control cannot recover the counterfactual without bias. Hybrid methods modify the constraints that govern this matching. Augmented synthetic control corrects for misestimated loadings through regression. Regularised variants shrink loadings toward zero or toward equality. Synthetic difference-in-differences accommodates common time trends that the basic factor structure might otherwise miss.

Return to the loyalty programme. The five flagship stores differ from controls not just in observable characteristics---square footage, foot traffic, product mix---but in unobservable ways that load onto latent factors. Perhaps urban flagship stores respond more strongly to macroeconomic shocks. Perhaps their customer base exhibits different seasonal patterns. Pure synthetic control tries to match on pre-treatment outcomes, hoping the match implicitly captures these factor loadings. When it fails, the treated unit sits outside the convex hull, and no combination of donors can replicate its trajectory.

Augmented synthetic control offers a fix. Fit the best synthetic control you can, then estimate a regression of outcomes on covariates within the control group. Apply that regression to adjust the synthetic control's predictions for the treated unit. The resulting estimator is \index{doubly robust}doubly robust: it remains consistent if either the weights or the regression model is correctly specified, provided standard regularity conditions hold. But doubly robust does not mean doubly correct. In small samples---and marketing panels are almost always small in the relevant dimension---both models contribute to finite-sample bias. The insurance policy has limits.

\index{regularisation}Regularisation addresses a different pathology. When the donor pool is large, synthetic control can overfit. The optimisation finds weights that match pre-treatment outcomes precisely, but those weights may be chasing noise. A donor that happens to correlate with the treated unit's idiosyncratic pre-treatment fluctuations receives weight it does not deserve. Out-of-sample, the synthetic control performs poorly. Ridge penalties shrink weights toward uniformity or toward zero. Elastic net combines sparsity with shrinkage. The bias-variance trade-off shifts: you accept some pre-treatment imbalance to avoid fitting noise.

\index{synthetic difference-in-differences}Synthetic difference-in-differences makes a structural innovation. Standard synthetic control constructs a weighted average of donor units. Synthetic DiD also constructs a weighted average of pre-treatment time periods. The estimator computes a double contrast: treated minus synthetic control, post-period minus weighted pre-period. This structure nests both difference-in-differences (equal unit weights, equal time weights) and synthetic control (optimised unit weights, equal time weights) as special cases. By optimising both sets of weights, synthetic DiD can handle settings where neither pure method would succeed.

Now consider the brand launching campaigns in twenty markets over three years. Markets adopt treatment at different times. Some start early, others late, some never. This is staggered adoption---a setting introduced in Chapter~\ref{ch:did}---and it complicates everything. You cannot simply pool post-treatment observations because early adopters contribute to the donor pool for late adopters. Standard aggregation methods can produce negative weights and perverse comparisons. Hybrid methods designed for staggered settings---extensions of synthetic DiD, cohort-specific augmented SC, panel matching with replacement---handle these structures explicitly. They define treatment effects at the cohort-time level, estimate them separately, and aggregate with transparent weights.

The design-based philosophy remains central, but it requires reinterpretation in hybrid settings. Design-based identification relies on the treatment assignment process, not functional forms. Pure synthetic control embodies this: the weights are chosen to match observables, and no regression model governs the outcome. Hybrid methods introduce modelling, which creates dependence on assumptions. The resolution is not to abandon design thinking but to use it as discipline. Choose models that respect the structure of the problem. Validate through diagnostics. Report sensitivity to specification. The goal is transparency about what you are assuming, not purity about what you are not.

What follows develops these ideas systematically. We begin with augmented synthetic control, showing how regression adjustment improves on pure weighting and when it helps most. We then examine regularised and balancing approaches that stabilise weights under different penalty structures. Synthetic difference-in-differences receives extended treatment: its construction, its identification assumptions, and its relationship to both DiD and SC. Subsequent sections address multiple treated units and staggered adoption, tuning and implementation, diagnostic workflows, inference, and applications. The loyalty programme and campaign launch examples recur throughout, grounding abstract methods in the practical realities of marketing measurement.

The core message is pragmatic. Hybrid methods are not a replacement for design. They are an extension---a way to make design-based inference work in settings where the data would otherwise defeat it. Use them when pure methods fail. Understand what you gain and what you give up. Report your choices transparently. That is how credible evidence gets built.
