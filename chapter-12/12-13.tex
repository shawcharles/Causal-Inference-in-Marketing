\section{Workflow Checklist}
\label{sec:dml-workflow}

This section provides a compact, reproducible protocol for conducting DML analyses in marketing panels. The workflow integrates estimand definition, nuisance estimation, cross-fitting, diagnostics, inference, and reporting, ensuring that conclusions are credible and transparent.

\subsection*{Step 1: Define Estimand}

Clarify the target parameter (e.g., ATE, ATT, CATE, or dose-response $\mu(w)$), ensuring consistency with the potential outcomes framework defined in Chapter~\ref{ch:frameworks}. Document the estimand mathematically and discuss its policy relevance. For staggered adoption, explicitly define the target aggregation: cohort-time effects $\text{ATT}(g, t)$, event-time effects $\theta_k$, or the global ATT.

\subsection*{Step 2: Construct Orthogonal Score}

Identify the nuisance functions required to estimate the target parameter: outcome regressions $\mu(w, x)$ (or $\mu_0(x), \mu_1(x)$ for binary treatment) and propensity scores $e(x)$ or treatment densities $f(w|x)$. Construct the doubly robust orthogonal score $\psi(\cdot)$ that combines these nuisances. Verify that the score satisfies Neyman orthogonality—first-order insensitivity to nuisance estimation error—typically via residual-on-residual constructions or AIPW augmentation.

\subsection*{Step 3: Design Dependence-Respecting Cross-Fitting}

Partition the data into $K$ folds (typically $K = 5$ or $K = 10$) while strictly respecting the panel dependence structure defined in Chapter~\ref{ch:frameworks}:
\begin{itemize}
    \item \textbf{Many units, few periods:} Use unit-level cross-fitting (partition unique identifiers).
    \item \textbf{Few units, many periods:} Use time-level cross-fitting (partition time blocks).
    \item \textbf{Balanced panels:} Use block cross-fitting (partition unit-time blocks).
\end{itemize}
Ensure that nuisance training data exclude post-treatment periods for treated units to prevent information leakage.

\subsection*{Step 4: Choose Learners and Tune}

Select ML learners for nuisance estimation based on the data structure: regularised GLMs (lasso, elastic net) for high-dimensional linear signals; gradient boosting or random forests for nonlinearities and interactions. Tune hyperparameters via cross-validation \textit{confined strictly to training folds}. Select hyperparameters that minimise out-of-sample prediction error (MSE for outcomes, log-likelihood/AUC for propensities).

\subsection*{Step 5: Assess Overlap and Trim}

Plot propensity score distributions for treated and control units. Compute the overlap statistic and flag extreme propensities (e.g., $e(x) < 0.05$ or $e(x) > 0.95$). Apply modest trimming (excluding the most extreme 1--5 per cent of observations) if propensities are concentrated at the boundaries. Report the effective sample size and verify that the trimmed sample remains representative of the target population.

\subsection*{Step 6: Estimate Effects with Clustered Inference}

For each fold $k$:
\begin{enumerate}
    \item Estimate nuisance functions on the $K-1$ training folds.
    \item Evaluate nuisances on the held-out fold.
    \item Construct the orthogonal score values $\hat{\psi}_{it}$ for the held-out data.
    \item Solve for the treatment effect estimate $\hat{\theta}_k$.
\end{enumerate}

Aggregate estimates across folds to obtain the final $\hat{\theta}$. Compute standard errors using the sample variance of the influence function (the orthogonal score), clustered by unit (or unit and time) to account for serial dependence. Report point estimates, standard errors, confidence intervals, and $p$-values as detailed in Chapter~\ref{ch:inference}.

\subsection*{Step 7: Conduct Diagnostics}

Run placebo tests in pre-treatment periods by applying the DML procedure to pseudo-intervention dates. Assess nuisance fit (out-of-sample $R^2$ or AUC) and balance improvement (SMDs). Check the stability of treatment effect estimates across folds and tuning grids. Compare DML estimates to design-based benchmarks (e.g., TWFE, Synthetic Control) to assess whether flexible functional forms materially change conclusions.

\subsection*{Step 8: Report Sensitivity and Policy Implications}

Report treatment effect estimates across multiple learner classes and tuning grids. Discuss which learner is most plausible based on fit diagnostics. For CATEs, report grouped effects and discuss policy implications: targeting rules, budget allocation, and heterogeneity mechanisms. Provide replication materials (data, scripts) to enable verification.

\begin{figure}[htbp]
\centering
\caption{Cross-Fitting Folds in Panels: Unit Blocks, Time Blocks, Two-Dimensional Blocks}
\label{fig:dml-crossfit}
\textit{[Figure to be inserted: Three-panel figure showing fold structures. Left: Unit-level cross-fitting (units on rows, time on columns, blocks partition rows). Middle: Time-level cross-fitting (blocks partition columns). Right: Block cross-fitting (grid partition). Annotations distinguish training vs. test folds.]}
\end{figure}

\begin{figure}[htbp]
\centering
\caption{Orthogonal Score Schematic for ATT with Nuisance Components}
\label{fig:dml-score}
\textit{[Figure to be inserted: Flowchart showing construction of doubly robust score. Inputs: $(Y, D, X)$. Nuisance Estimation: $\hat{\mu}(0,X)$ and $\hat{e}(X)$. Residuals: $Y - \hat{\mu}$ and $D - \hat{e}$. Combination: Weighted residual + regression adjustment $\to$ Score $\psi$. Solution: $\sum \psi = 0 \to \hat{\tau}$.]}
\end{figure}

\begin{figure}[htbp]
\centering
\caption{CATE Stability Across Folds and Tuning Grids}
\label{fig:dml-stability}
\textit{[Figure to be inserted: Left: Scatter plot of CATE estimates from Fold $k$ vs. Fold $k'$ (45-degree line indicates stability). Right: Line plot of Grouped CATEs (y-axis) vs. Tuning Parameter $\lambda$ (x-axis) for distinct subgroups.]}
\end{figure}

\begin{table}[htbp]
\begin{tighttable}
\centering
\caption{Mapping from Estimands to Nuisance Components, Scores, and Aggregation}
\label{tab:dml-estimands}
\begin{tabularx}{\textwidth}{Y Y Y Y}
\toprule
\textbf{Estimand} & \textbf{Nuisance Components} & \textbf{Orthogonal Score} & \textbf{Aggregation} \\
\midrule
ATE (High-Dim) & $\mu_1(x), \mu_0(x), e(x)$ & AIPW / Doubly Robust & Average $\psi_i$ \\
\addlinespace
ATT (Staggered) & $\mu_g(t, x), e_g(x)$ & Cohort-time residuals + weighting & Weight $\text{ATT}(g,t)$ to $\theta_k$ \\
\addlinespace
Dose-Response & $\mu(w, x), f(w \mid x)$ & Residual-on-residual / Riesz representer & Marginal effect via derivative \\
\addlinespace
CATE $\tau(x)$ & $\mu_1(x), \mu_0(x), e(x)$ & Doubly Robust Scores & Causal Forest / BLP \\
\addlinespace
Policy Value & $\tau(x)$, Cost $c$ & Value Function $\psi_{\pi}$ & Average under policy $\pi$ \\
\bottomrule
\end{tabularx}
\end{tighttable}
\end{table}

\begin{quote}
\textbf{Box 12.1: DML Workflow Checklist}

\textbf{1. Define Estimand:} Specify ATE, ATT, CATE, or dose-response. Define aggregation for staggered adoption.

\textbf{2. Construct Orthogonal Score:} Identify nuisances. Derive doubly robust score $\psi$. Verify Neyman orthogonality.

\textbf{3. Design Cross-Fitting:} Partition data ($K$-fold) respecting panel dependence (unit, time, or block). Avoid leakage.

\textbf{4. Choose Learners and Tune:} Select ML methods (Lasso, GBM, Forests). Tune via CV on training folds only.

\textbf{5. Assess Overlap:} Check propensity distributions. Trim extremes (1--5\%). Verify covariate balance.

\textbf{6. Estimate and Infer:} Estimate nuisances, construct scores, and aggregate $\hat{\theta}$. Cluster standard errors by unit/time.

\textbf{7. Diagnostics:} Run placebo tests. Check nuisance fit and estimate stability. Compare with baseline designs.

\textbf{8. Sensitivity \& Policy:} Report estimates across learners. Analyze heterogeneity (CATE) and policy implications.
\end{quote}
\index{double machine learning|)}
\index{CATE|)}
