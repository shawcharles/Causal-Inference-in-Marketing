
\section{Double/Debiased ML Estimators in Panels}
\label{sec:dml-estimators}

This section develops double/debiased machine learning estimators for panel data, providing formal convergence results for the ATT and extending to staggered adoption designs common in marketing.

Before stating the main result, we require an overlap condition ensuring that treated and control observations share common support.

\begin{assumption}[Overlap]\label{ass:dml-overlap}
The propensity score is bounded away from zero and one: there exists $\epsilon > 0$ such that $\epsilon < e(x) < 1 - \epsilon$ for all $x$ in the support of $X$.
\end{assumption}

Overlap ensures that for any covariate value observed among treated units, we can find comparable control units to construct counterfactuals. Violations of overlap lead to extreme weights and unstable estimates.

\begin{theorem}[DML Asymptotic Theory]\label{thm:dml-asymptotic}
Under Assumptions~\ref{ass:nuisance-rates}, \ref{ass:panel-crossfit}, and \ref{ass:dml-overlap}, the cross-fitted DML estimator $\hat{\tau}$ for the ATT satisfies:
\begin{enumerate}[(i)]
    \item \textbf{Consistency:} $\hat{\tau} \xrightarrow{p} \tau_0$ as $N \to \infty$;
    \item \textbf{Asymptotic normality:}
    \[
    \sqrt{N}(\hat{\tau} - \tau_0) \xrightarrow{d} \mathcal{N}(0, V),
    \]
    where $V = \mathbb{E}[\psi^{\text{ATT}}(Z; \tau_0, \eta_0)^2]$ is the influence function variance;
    \item \textbf{Bias bound:} The bias satisfies $|\mathbb{E}[\hat{\tau}] - \tau_0| \leq C \cdot \|\hat{\mu}_0 - \mu_0\| \cdot \|\hat{e} - e\| = o(N^{-1/2})$.
\end{enumerate}
The rate $\sqrt{N}$ reflects that inference is over $N$ independent units (clusters), not $n = NT$ observations. Within-unit dependence across time periods is absorbed into the variance $V$.
\end{theorem}

Inference relies on consistent variance estimation. Because observations within a unit are dependent across time, we must cluster standard errors at the unit level.

\begin{proposition}[Cluster-Robust Variance Estimator]\label{prop:variance-dml}
Under panel dependence with clustering by unit, a consistent estimator of the asymptotic variance $V$ is:
\[
\hat{V} = \frac{1}{N} \sum_{i=1}^N \left(\sum_{t=1}^T \psi^{\text{ATT}}(Z_{it}; \hat{\tau}, \hat{\eta}^{(-k(i))})\right)^2,
\]
where $k(i)$ denotes the fold containing unit $i$, and the inner sum aggregates influence contributions across all periods for unit $i$. Then $\sqrt{N}(\hat{\tau} - \tau_0) / \sqrt{\hat{V}} \xrightarrow{d} \mathcal{N}(0, 1)$.
\end{proposition}

\subsection*{Direct Debiased ML and Bregman-Riesz Regression}

Double machine learning is often introduced as a generic recipe in which we choose flexible learners for the regression function and the propensity score, plug them into an orthogonal score, and cross fit. Recent work by Kato \citet{kato2025unified} shows that there is more structure to exploit. Many familiar estimators, including inverse probability weighting, doubly robust scores, covariate balancing and targeted maximum likelihood, can be viewed through a single object, the Riesz representer $\alpha(W, X)$. In the ATT example this is the bias correction term that re-weights residuals in Definition~\ref{def:dr-score}. Estimating $\alpha$ well is as central as estimating $\mu_0$, because $\alpha$ determines which regions of covariate space receive weight when we construct the score.

Bregman-Riesz regression treats $\alpha$ as the solution to an optimisation problem built from a Bregman divergence. Squared-loss Bregman-Riesz regression reproduces the Riesz regression used in automatic debiased machine learning, which coincides with least-squares density-ratio estimation in simple settings. Alternative convex generators such as Kullback--Leibler losses recover entropy-balancing style estimators, where the dual problem matches covariate moments exactly. From this perspective inverse probability weighting, entropy balancing, stable balancing weights and related procedures are different ways of targeting the same orthogonal score with different choices of loss and model class for $\alpha$.

For applied work this unified view has two practical implications. First, it encourages you to think explicitly about the weighting scheme implicit in your DML implementation rather than treating the choice of loss and basis for $\alpha$ as a black box. Second, it suggests that you can borrow regularisation and diagnostics from the density-ratio literature when fitting Riesz regressions, for example by inspecting whether estimated weights concentrate on a small number of observations. In marketing panels where extreme weights correspond to a handful of unusual stores or customers, combining DML with stable Bregman-Riesz objectives delivers more stable estimates while preserving the orthogonality and efficiency properties emphasised in this chapter.

\subsection*{Staggered Adoption and DML-DiD}

Marketing interventions rarely occur simultaneously across all units. Loyalty programmes roll out region by region, advertising campaigns launch in waves, and pricing experiments stagger across stores. This staggered adoption creates multiple treatment cohorts, each defined by its adoption time $G_i$. The DML framework extends naturally to this setting by estimating cohort-time-specific effects and aggregating them.

\begin{theorem}[DML-DiD with Staggered Adoption]\label{thm:dml-did}
For staggered adoption with cohorts $g \in \mathcal{G}$ and comparison group $\mathcal{C}$ (never-treated or not-yet-treated units), define cohort-time effects:
\[
\text{ATT}(g, t) = \mathbb{E}[Y_{it}(g) - Y_{it}(\infty) | G_i = g], \quad t \geq g,
\]
where $Y_{it}(g)$ denotes the potential outcome for a unit adopting at time $g$, and $Y_{it}(\infty)$ denotes the potential outcome under never-treatment. Under conditional parallel trends and Assumptions~\ref{ass:nuisance-rates}--\ref{ass:panel-crossfit}:
\begin{enumerate}[(i)]
    \item The DML estimator $\widehat{\text{ATT}}(g, t)$ using cohort-specific nuisances is $\sqrt{N_g}$-consistent;
    \item Event-time aggregation $\hat{\theta}_k = \sum_g \omega_g \widehat{\text{ATT}}(g, g+k)$ with weights $\omega_g \propto N_g$ is asymptotically normal:
    \[
    \sqrt{N}(\hat{\theta}_k - \theta_k) \xrightarrow{d} \mathcal{N}(0, V_k),
    \]
    where $V_k$ accounts for correlation across cohorts sharing comparison units.
\end{enumerate}
\end{theorem}

\subsection{DML in Panel Settings: The Role of Unobserved Heterogeneity}
Applying DML to panel data requires care in handling unobserved unit heterogeneity $\alpha_i$. A naive approach might time-demean the outcome and covariates (as in linear fixed effects) and then apply DML. However, \citet{fuhr2024double} demonstrate that this fails when the true relationship between covariates and outcomes is nonlinear. Time-demeaning removes $\alpha_i$ only under strict additivity and linearity.

The Correlated Random Effects (CRE) approach, also known as the Mundlak device, provides a solution. For every time-varying covariate $X_{it}$, we augment the feature set with its unit-level time average $\bar{X}_i = T^{-1}\sum_t X_{it}$. The ML learner then estimates $Y_{it} = f(X_{it}, \bar{X}_i) + \varepsilon_{it}$, where the function $f$ can be arbitrarily nonlinear. By conditioning on $\bar{X}_i$, the CRE approach proxies for the unobserved heterogeneity $\alpha_i$ without requiring the strict additivity assumption of the within-transformation. Intuitively, $\bar{X}_i$ captures the permanent component of the covariate that correlates with $\alpha_i$, while $X_{it} - \bar{X}_i$ captures the transitory variation that identifies the causal effect.

This method performs significantly better in simulations with nonlinear confounding \citet{fuhr2024double} and is the standard for rigorous panel DML applications in marketing, such as estimating price elasticities with store-level heterogeneity or measuring advertising effects with brand-level unobservables.

\subsection*{Semi-Supervised DML with Unlabelled Covariates}

The Riesz regression viewpoint also clarifies how to exploit unlabelled covariates when treatments and outcomes are scarce. In many marketing databases we have detailed profiles for large numbers of users or stores but observe assignment and revenue outcomes only for a managed test sample. Kato \citet{kato2025semi} studies such settings and shows that, under suitable conditions, using auxiliary observations of $X$ alone can reduce the asymptotic variance of average treatment effect estimators relative to procedures that ignore them.

The key idea is to treat unlabelled covariates as additional information about the distribution of $X$ that enters the estimation of the Riesz representer. In the one-sample or censoring design, a single dataset contains both fully observed units and units for which $W$ and $Y$ are missing. In the two-sample or case-control design, we observe one dataset with complete triples $(X, W, Y)$ and a second dataset with $X$ only. In both cases the efficient influence function still has the usual doubly robust form, but the optimal weighting function $\alpha$ now depends on the joint law of $X$, $W$, and the observation process. Generalised Bregman-Riesz regression uses both labelled and unlabelled covariates when minimising its loss, so the estimated weights exploit all available information about covariate frequencies.

For most everyday applications in this book you can treat the labelled sample as the relevant universe and apply standard panel DML. The semi-supervised perspective becomes valuable when you truly have a much larger pool of passive covariate data, for example platform level browsing histories or impression logs, but can only measure outcomes and treatment on a subset because of logging costs or privacy constraints. In those cases semi-supervised DML provides a principled way to translate abundant information about $X$ into tighter confidence intervals for treatment effects rather than simply using the extra data for prediction.

\textbf{PUATE and positive--unlabelled designs.} A related but distinct scenario arises when outcomes are observed widely but treatment status is only known for a subset of units. Kato, Kozai and Inokuchi \citet{kato2025puate} study this positive--unlabelled ATE problem, where we observe a confirmed treatment group and a large unknown group that mixes treated and untreated units. They treat this as an analogue of positive--unlabelled classification and derive semiparametric efficiency bounds and efficient influence functions under both censoring and case--control sampling schemes. The resulting estimators combine observation models, propensity scores and outcome regressions in a way that resembles doubly robust scores but with a modified double robustness structure, because the propensity for appearing in the labelled treatment group plays a central role. In marketing terms, PUATE covers designs where online or logged exposures create a known treated cohort, while offline or unlogged channels feed into an unknown mixture whose treatment status is not directly observed. When those designs are credible and the additional PU learning assumptions are defensible, PUATE-type estimators offer a principled way to recover ATEs from treated and unlabelled units rather than discarding large swathes of partially labelled outcome data.
