\section{Estimation under Staggered Adoption}
\label{sec:event-estimation}

Estimating event-time effects under staggered adoption with heterogeneous treatment effects requires care to avoid the biases that plague traditional two-way fixed effects (TWFE) event-study regressions. This section explains the sources of bias, presents heterogeneity-robust estimation methods, and provides practical guidance on implementation in marketing panels.

\subsection*{TWFE Bias in Event Studies}

TWFE event studies can suffer contamination by already-treated units and negative weights under heterogeneity. The core problem is that TWFE uses already-treated units as implicit controls when estimating effects for newly-treated units, comparing units at event time $k=2$ to units at $k=5$. When effects evolve over event time, this produces contaminated comparisons that mix treatment effect dynamics with differential timing. Additionally, cohort-specific effects enter with opaque regression weights that need not be proportional to cohort size and can be negative on some cohort-time cells.

For formal derivations, see Chapter~\ref{ch:did} (Sections~\ref{sec:twfe-pitfalls} and \ref{sec:event-dynamics}). Use TWFE plots as diagnostics to compare against heterogeneity-robust estimators, and rely on heterogeneity-robust estimators for primary estimation.

\subsection*{Heterogeneity-Robust Estimators}

Construct clean comparisons using never-treated or not-yet-treated controls. Two common choices are:

The Sun--Abraham interaction-weighted regression:
\[
Y_{it} = \alpha_i + \beta_t + \sum_{g} \sum_{k \neq -1} \delta_{g,k} \mathbb{I}\{G_i = g, \, t - G_i = k\} + \varepsilon_{it}.
\]
This estimates cohort-specific paths $\delta_{g,k}$ and aggregates them into $\hat{\theta}_k = \sum_g \hat{\omega}_{g,k} \hat{\delta}_{g,k}$ with user-specified cohort weights.

The Callaway--Sant'Anna aggregation from group-time effects:
\[
\hat{\theta}_k = \sum_g \omega_{g,k} \, \widehat{\tau}(g, g + k),
\]
See Chapter~\ref{ch:did}, Section~\ref{sec:modern-estimators} for derivations and properties.

\textbf{Estimator Selection Guide.}
\begin{center}\small
\begin{tabular}{p{4cm}p{4cm}p{5cm}}
\toprule
\textbf{If your goal is...} & \textbf{Use...} & \textbf{Rationale} \\
\midrule
Event-time profile with pre-trend tests & Sun--Abraham & Directly estimates $\theta_{g,k}$ for each cohort and event time; clean pre-treatment coefficients \\
\addlinespace
Flexible aggregation across cohorts & Callaway--Sant'Anna & Separates estimation from aggregation; allows custom weighting schemes \\
\addlinespace
Settings with factor structure & Borusyak--Jaravel--Spiess & Imputation-based: estimates counterfactuals for treated observations using untreated data, then computes effects as residuals and accommodates interactive fixed effects \\
\addlinespace
Quick benchmark & TWFE & Fast but potentially biased, so always compare to robust estimators \\
\bottomrule
\end{tabular}
\end{center}

\noindent Software: R packages \texttt{did}, \texttt{fixest} (with \texttt{sunab()}), \texttt{did2s}, and Stata commands \texttt{csdid} and \texttt{eventstudyinteract}. Compare methods for robustness.

\textbf{Estimator choice can affect results.} Different heterogeneity-robust estimators can produce different point estimates even when all are correctly implemented, because they target slightly different estimands or use different weighting schemes. Sun--Abraham and Callaway--Sant'Anna may differ if they use different control groups (never-treated vs. not-yet-treated) or different aggregation weights. Reporting results from multiple estimators provides evidence on robustness. Large discrepancies warrant investigation into the source of the difference (control group, weights, or underlying heterogeneity).

\subsection*{Control Set and Weighting Choices}

Control set choices---whether to use never-treated units only or to include not-yet-treated units---affect identification and precision. Using never-treated units as controls is straightforward and transparent but requires that never-treated units exist and are comparable to treated units. If all units eventually adopt treatment, never-treated controls are unavailable, and identification must rely on not-yet-treated controls. Not-yet-treated controls are units that have not yet been treated by the period in question but will be treated later. Using not-yet-treated controls increases the effective sample size and can improve precision.

\textbf{Stronger assumption for not-yet-treated controls.} Using not-yet-treated controls requires that parallel trends holds not only between treated and never-treated units but also between early and late adopters. This is a stronger assumption than using never-treated controls alone. If late adopters differ systematically from early adopters---for example, if the intervention is rolled out first to high-performing units or to units with the greatest expected benefit---then parallel trends between early and late adopters may be implausible. Never-treated controls are then preferable despite the loss of precision. When both control groups are available, comparing results using never-treated only vs. never-treated plus not-yet-treated provides a robustness check.

Weighting conventions matter for interpretation. Cohort-size weights give each cohort weight proportional to the number of treated units in that cohort, reflecting the prevalence of the cohort in the treated population. Treated-unit-period weights give each cohort weight proportional to the number of treated unit-periods it contributes at event time $k$, reflecting the density of observations. Uniform weights give each cohort equal weight regardless of size. Different weighting schemes produce different $\theta_k$ estimates when effects are heterogeneous across cohorts. The choice should be pre-specified based on the substantive question (whether large cohorts should dominate the aggregation or whether all cohorts should be weighted equally) and should be reported transparently.

\subsection*{Implementation Guidance}

Implementation in marketing panels requires attention to data structures and diagnostic checks. Thin panels with many units and few periods are common in retail, and event-study windows must be chosen to respect the limited time horizon. Fat panels with few units and many periods enable longer event-time windows but may have few cohorts, limiting the ability to estimate cohort-specific profiles. Unbalanced panels with entry, exit, or missing observations require care to ensure that event-time indicators are well defined and that support is adequate. Pre-specifying the event-time window, binning scheme, control set, and weighting convention in a pre-analysis plan disciplines the analysis and guards against specification searches. Reporting the support, cohort composition, and estimated weights for each event time provides transparency and enables readers to assess robustness.

The modern event-study framework clarifies that event studies are not merely a graphical tool but a principled estimation strategy grounded in the potential outcomes framework (Chapter~\ref{ch:frameworks}). By making explicit the target estimands, the comparison groups, and the aggregation schemes, heterogeneity-robust event-study methods produce credible causal estimates that withstand scrutiny and inform strategic decisions with confidence. The next section articulates the identification assumptions that underpin these methods.
