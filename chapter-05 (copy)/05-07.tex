\section{Inference}
\label{sec:event-inference}

Valid inference for event-time effects requires accounting for error correlation, multiplicity across event times, and finite-sample properties. We outline procedures with forward references to Chapter~\ref{ch:inference}.

\subsection*{Clustering}

\index{clustering}Clustering is the default approach to accounting for within-unit correlation over time. Outcomes for the same unit in different periods are correlated due to persistent unobservables, autocorrelated shocks, or dynamic feedback. Clustering standard errors by unit allows for arbitrary correlation within units while maintaining independence across units. The cluster-robust variance estimator is valid asymptotically as the number of units grows, provided that errors are uncorrelated across units. This independence assumption is often violated in marketing settings: regional shocks affect multiple stores, competitive responses propagate across firms, and platform algorithm changes affect all users simultaneously. When cross-unit correlation is plausible, two-way clustering or other approaches are required.

\index{clustering!two-way}Two-way clustering, by both unit and time, accounts for correlation within units over time and across units within periods. If all stores in a region are affected by a regional shock, errors are correlated across stores within a period. If macroeconomic conditions or platform algorithm changes affect all units in a given period, errors are correlated across units. Two-way clustering captures both sources of correlation, producing standard errors that are valid under weak assumptions. The cost is substantially larger standard errors---often 2--3 times larger than one-way clustering---and greater computational complexity, though modern software implements two-way clustering efficiently. The increase in standard errors reflects genuine uncertainty that one-way clustering ignores; it is not a defect of the method.

\subsection*{Small-Sample Corrections}

Small-$G$ concerns arise when the number of clusters (treated cohorts or units) is modest. Cluster-robust standard errors rely on large-cluster asymptotics, which may not provide accurate inference when the number of clusters is small. The severity of the problem depends on the cluster count. With fewer than 20 clusters, asymptotic cluster-robust standard errors are unreliable, and the wild cluster bootstrap or randomisation inference should be used instead. With 20 to 50 clusters, asymptotic standard errors may be adequate but should be checked against bootstrap results, and small-sample corrections (for example, HC2 or HC3 variants) should be applied. With 50 or more clusters, asymptotic cluster-robust standard errors are generally reliable.

In marketing panels, the number of treated cohorts or the number of DMAs in a geo-experiment may be modest, making asymptotic approximations unreliable. The \index{wild cluster bootstrap}wild cluster bootstrap provides finite-sample inference by resampling entire clusters, imposing random signs on cluster-level residuals, and computing the bootstrap distribution of the test statistic. It respects the clustering structure and accommodates heteroskedasticity and serial correlation. This provides more accurate p-values and confidence intervals than asymptotic methods when clusters are few. The computational cost of the wild cluster bootstrap is non-trivial---typically requiring 999 or more bootstrap replications---but modern software handles this efficiently for most panel sizes.

\subsection*{Randomisation Inference}

\index{randomisation inference}Randomisation inference and permutation tests offer design-based alternatives that do not rely on parametric assumptions about error distributions. Under the sharp null hypothesis of no effect for any unit at any event time, the observed treatment assignment is just one of many possible assignments that could have been drawn from the randomisation protocol. By recomputing the test statistic for all possible assignments (or a large random sample of them), we generate the exact null distribution of the test statistic. We then compare the observed statistic to this distribution to compute an exact p-value. Randomisation inference is particularly compelling in experimental settings where the randomisation protocol is known and where the goal is to conduct inference that respects the design.

\textbf{Limitation.} Randomisation inference requires knowledge of the randomisation protocol, which is often unavailable in observational settings. When treatment assignment is not randomised or when the assignment mechanism is unknown, randomisation inference is not applicable. In such cases, cluster-robust standard errors or bootstrap methods remain the default, with the caveat that they rely on asymptotic approximations and modelling assumptions.

\subsection*{Multiple Testing}

Multiple testing arises because event studies estimate effects for many event times $k$, and testing whether $\theta_k \neq 0$ for each $k$ involves multiple hypothesis tests. If we test 15 event-time coefficients at the 5 per cent significance level, the probability of at least one false rejection (family-wise error rate) exceeds 5\% unless we adjust for multiplicity. Bonferroni correction controls the family-wise error rate by dividing the significance level by the number of tests. However, this is conservative when tests are correlated (as event-time coefficients typically are, because they are estimated from overlapping data). False discovery rate (FDR) control offers a less conservative alternative, controlling the expected proportion of false discoveries among all rejections. Romano--Wolf stepdown procedures exploit the correlation structure of the test statistics to improve power while controlling family-wise error rate (see Chapter~\ref{ch:inference} for details and implementation).

Families of hypotheses should be defined ex ante to guide multiplicity adjustments. One natural family is the set of pre-treatment coefficients $\{\theta_k : k < 0\}$, which jointly test for pre-trends and anticipation. A joint Wald test with cluster-robust standard errors that all pre-treatment coefficients are zero provides a single hypothesis test at the desired significance level (a standard F-test assumes homoskedasticity and is inappropriate when errors are clustered). This avoids the need for multiplicity adjustment across individual pre-treatment coefficients. Another family is the set of post-treatment coefficients $\{\theta_k : k \geq 0\}$, which jointly test for any treatment effect. If the goal is to test whether the intervention has any effect at any post-treatment event time, a joint test provides a single hypothesis test. If the goal is to estimate and report effects at each post-treatment event time, multiplicity adjustments (Bonferroni, FDR, Romano--Wolf) may be appropriate. Many researchers report unadjusted p-values and confidence intervals for individual event times and rely on joint pre-trend tests to establish credibility.

\subsection*{Practical Guidance}

Practical guidance for marketing applications includes clustering by unit as a default, two-way clustering when cross-unit correlation is plausible (for example, in geo-experiments or when regional shocks are present), wild cluster bootstrap when the number of clusters is small (fewer than 50), and multiplicity adjustments when testing many event-time coefficients. Pre-specifying the primary estimand (overall ATT, cumulative effect, or specific event-time coefficients of substantive interest) and distinguishing primary from exploratory analyses reduces the multiplicity burden. If the primary estimand is the cumulative effect and individual event-time coefficients are exploratory, then only the cumulative effect test needs to control type I error at the nominal level. Event-time tests can be reported without adjustment as exploratory evidence.

Transparency about inferential choices and robustness checks using alternative methods build confidence in conclusions. Report both unadjusted and adjusted p-values or confidence intervals. Document the families of hypotheses tested. Discuss the trade-offs between controlling family-wise error rate and power.

\textbf{Inference choices can affect conclusions.} Different clustering choices, small-sample corrections, or multiplicity adjustments can lead to different conclusions about statistical significance. An effect that is significant with one-way clustering may become insignificant with two-way clustering. An effect that is significant without multiplicity adjustment may become insignificant after Bonferroni correction. Sensitivity analysis across inferential choices---reporting results under multiple specifications---provides evidence on the robustness of conclusions. If conclusions are sensitive to inferential choices, this should be acknowledged rather than hidden.

This ensures that readers understand the evidential standard applied and can assess whether alternative inferential choices would change conclusions. The next section develops diagnostic workflows for event-study analyses.
