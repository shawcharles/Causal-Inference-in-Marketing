\section{Assignment Mechanisms and Identification}
\label{sec:assignment-mechanisms}

How did units get treated? Your answer determines which methods you can credibly use and what assumptions you must defend.

Treatment assignment in observational marketing data is rarely random. Firms choose which stores receive loyalty programmes based on expected profitability. Advertisers target campaigns to markets with high anticipated returns. Platforms enter cities based on market size and competitive conditions. These endogenous decisions mean that treated and control units differ systematically, and the observed difference in outcomes conflates the causal effect of treatment with selection bias.

Causal panel data methods overcome this challenge by combining substantive knowledge of the assignment mechanism with data structures --- repeated observations, staggered timing, common shocks --- that enable identification under weaker assumptions than cross-sectional settings require. This section classifies common assignment mechanisms and maps them to the identification assumptions that justify particular estimators.

\subsection*{Randomised Assignment}

Randomised assignment is the gold standard of causal inference. It ensures that treatment is independent of potential outcomes: $W_{it} \perp (Y_{it}(0), Y_{it}(1))$. Because treatment and potential outcomes are independent, the difference in observed outcomes between treated and control units has a causal interpretation. No further assumptions about functional forms, parallel trends, or factor structures are required.

In marketing, randomised assignment typically takes the form of geo-experiments, where DMAs or cities are randomly assigned to treatment and control conditions. A brand might randomly assign 25 DMAs to receive a TV advertising campaign while 25 DMAs serve as controls, then compare sales across conditions. The random assignment ensures that any difference in post-campaign sales reflects the causal effect of advertising, not pre-existing differences between markets.

Chapter~\ref{ch:design-overview} discusses randomised experiments in more depth, covering geo-experiments, switchback experiments, and A/B tests. Despite their appeal, experiments face limitations: spillovers between treatment and control units can bias estimates, short durations miss long-run effects, and cost or ethical concerns may preclude randomisation. Even when experiments are feasible, panel methods remain valuable --- they control for pre-treatment imbalances, extend short-term results with observational follow-up, and estimate heterogeneous effects.

\subsection*{Staggered Adoption}

When randomisation is infeasible, as is often the case in marketing, we must rely on observational variation in treatment timing. Staggered adoption occurs when units adopt treatment at different times. Some units adopt in period 2 (cohort $g = 2$), others in period 5 (cohort $g = 5$), and some never adopt during the observation window ($g = \infty$). This structure is ubiquitous in marketing: a retailer rolls out a loyalty programme to batches of stores over multiple quarters, a brand launches advertising campaigns sequentially across markets, a platform enters cities in staggered fashion. The variation in adoption timing creates opportunities for identification provided that units adopting at different times would have followed parallel trends in the absence of treatment. The parallel trends assumption asserts that differences in outcome trajectories between units with different adoption times can be attributed to treatment rather than to differential pre-existing trends.

Formally, we state this as an assumption.

\begin{assumption}[Parallel Trends]
\label{assump:parallel-trends}
For all cohorts $g, g' \in \{1, \ldots, T\} \cup \{\infty\}$ and all periods $t < \min(g, g')$ (i.e., before either cohort is treated),
\[
\mathbb{E}[Y_{it}(\infty) - Y_{i,t-1}(\infty) \mid G_i = g] = \mathbb{E}[Y_{it}(\infty) - Y_{i,t-1}(\infty) \mid G_i = g'],
\]
where $Y_{it}(\infty)$ denotes the potential outcome under never receiving treatment.
\end{assumption}

In words, the expected change in untreated potential outcomes from one period to the next is the same across cohorts. This assumption is stated in terms of changes rather than levels because it allows cohorts to differ in their baseline outcome levels --- early adopters (those with small $g$, adopting in early calendar periods) might have systematically higher sales than late adopters --- while still requiring that their growth rates would have been parallel absent treatment. This assumption does not require that levels or slopes are identical across cohorts, only that the evolution over time --- absent treatment --- would have been parallel. Staggered adoption designs are particularly compelling when the timing of adoption is driven by factors unrelated to the anticipated magnitude of the treatment effect. If the firm rolls out a programme alphabetically by store name, or if a platform enters cities based on operational capacity constraints rather than expected profitability, then parallel trends may be plausible. Chapter~\ref{ch:did} develops modern heterogeneity-robust estimators that aggregate $\tau(g, t)$ effects from staggered designs, and Chapter~\ref{ch:event} discusses event-study specifications that test for pre-trends and estimate dynamic effects.

\subsection*{Single Treated Unit}

In some cases, treatment variation is even more limited. When only one unit is treated while all others serve as controls --- for example, a platform launching in a single test city while comparable cities remain untreated, or a firm implementing a major strategic change in one market --- we cannot rely on variation in treatment timing. Single-unit designs call for constructing a synthetic version of the treated unit from a weighted combination of control units, as in the synthetic control method (Chapter~\ref{ch:sc}). The synthetic control is chosen such that its pre-treatment outcomes closely match the treated unit's pre-treatment outcomes, under the assumption that if pre-treatment fit is sufficiently close (typically assessed by examining pre-treatment mean squared prediction error), then the synthetic control provides a valid counterfactual for the post-treatment period.

\subsection*{Common Shocks and Differential Exposure}

A related but distinct setting arises when all units experience a common event in the same period but with varying intensity or exposure. For example, a national advertising campaign might reach different markets with varying gross rating points, a regulatory change might affect firms differently based on their characteristics, or a platform algorithm update might impact sellers with different product portfolios to different degrees. These designs exploit cross-sectional variation in exposure intensity, comparing outcomes before and after the shock across units. Identification relies on parallel trends --- units with different exposure levels would have evolved similarly absent the shock --- or more flexible structures such as interactive fixed effects (Chapter~\ref{ch:factor}) that allow for heterogeneous responses to common time-varying factors.

\subsection*{Continuous Treatment Intensity}

Treatment assignment need not be binary. In many marketing applications, treatment varies in intensity. The TV advertising example from Chapter 1 illustrates this: the brand varies GRPs across markets and weeks, and we want to know how sales respond to changes in advertising intensity. Promotional discount depth, loyalty programme reward generosity, and pricing all take continuous values.

The potential outcomes framework extends naturally: $Y_{it}(w)$ denotes the potential outcome under treatment level $w$. The causal effect of moving from treatment intensity $w$ to $w'$ is $Y_{it}(w') - Y_{it}(w)$. For the TV advertising example, we might ask: what is the effect of increasing GRPs from 100 to 150 in a given market-week? The dose-response function traces out how outcomes change across the full range of treatment intensities.

Identification typically relies on a conditional independence assumption, which we state formally.

\begin{assumption}[Unconfoundedness for Continuous Treatment]
\label{assump:unconfoundedness-continuous}
Conditional on covariates $X_{it}$, unit fixed effects $\alpha_i$, and time fixed effects $\lambda_t$, the treatment intensity is independent of potential outcomes:
\[
W_{it} \perp Y_{it}(w) \mid X_{it}, \alpha_i, \lambda_t \quad \text{for all } w \in \mathcal{W},
\]
where $\mathcal{W}$ denotes the support of treatment intensity.
\end{assumption}

This is a strong assumption. It requires that we observe and control all confounders---all factors that jointly affect both advertising spending and sales.

Panel settings make this assumption more plausible by including unit and time fixed effects, which control for time-invariant unobservables and common shocks. High-dimensional controls (Chapter~\ref{ch:high-dim}) and double machine learning (Chapter~\ref{ch:ml-nuisance}) provide flexible approaches to conditioning on many covariates without imposing restrictive functional forms. Dose-response functions, marginal effects, and elasticity estimation for continuous treatments are covered in Chapter~\ref{ch:continuous}.

\subsection*{Combining Identification Strategies and Method Selection}

In practice, you should combine identification strategies rather than treat them as mutually exclusive. Difference-in-differences can incorporate high-dimensional controls; synthetic control can be followed by sensitivity analyses; factor models can be combined with staggered adoption to accommodate common shocks and timing. What matters is matching the design to the variation that identifies the effect.

\subsubsection*{Parallel Trends Revisited}

Parallel trends (Assumption~\ref{assump:parallel-trends}) asserts that treated and control units would have evolved similarly absent treatment. It can be stated conditionally (after covariate adjustment) or unconditionally (in levels). Conditional versions are often more plausible; unconditional versions require fewer modelling choices. Although untestable directly, pre-treatment fit and placebo checks (Chapter~\ref{ch:design-diagnostics}) provide indirect evidence. Difference-in-differences and event studies (Chapters~\ref{ch:did}, \ref{ch:event}) rely on this assumption, and recent sensitivity analyses \citet{rambachan2023more} quantify how large a violation must be to overturn conclusions.

\subsubsection*{Factor Structure}

Factor structure provides an alternative when parallel trends in levels is implausible but units are subject to common time-varying shocks that affect them differentially. While parallel trends requires that treated and control units evolve similarly, factor models allow for heterogeneous responses to common shocks. Interactive fixed effects models (Chapter~\ref{ch:factor}) posit that untreated potential outcomes can be decomposed as
\[
Y_{it}(0) = \sum_{r=1}^R \lambda_{ir} f_{tr} + \varepsilon_{it},
\]
where $f_{tr}$ are latent factors common to all units and $\lambda_{ir}$ are unit-specific loadings. This structure accommodates differential exposure to common shocks -- such as seasonality, macroeconomic trends, or industry-wide demand shifts -- without requiring parallel trends. Estimation proceeds via principal components, expectation-maximisation algorithms, or nuclear norm regularisation. Factor models are particularly effective in marketing panels where all stores are affected by category demand shocks, all markets experience national advertising campaigns, or all platforms face common technological changes.

\subsubsection*{Unconfoundedness with High-Dimensional Controls}

Unconfoundedness (Assumption~\ref{assump:unconfoundedness-continuous}) applies in settings where treatment intensity varies continuously or where treatment is targeted based on observable characteristics. Conditional independence assumptions can justify causal inference when we control for a sufficiently rich set of covariates -- demographics, past outcomes, competitor actions, seasonal indicators -- such that treatment assignment is as good as random conditional on these controls. This is a strong assumption -- it requires that there are no unobserved confounders that jointly affect treatment and outcomes -- but panel structures strengthen this strategy by allowing unit fixed effects (controlling for time-invariant confounders) and time fixed effects (controlling for common shocks), reducing the set of potential unobserved confounders to time-varying unit-specific factors. When the covariate set is high-dimensional, regularisation methods such as lasso (Chapter~\ref{ch:high-dim}) can select relevant controls without overfitting, and double machine learning (Chapter~\ref{ch:ml-nuisance}) enables valid inference even when the selection and outcome models are estimated flexibly.

\subsubsection*{Interference-Aware Designs}

Interference-aware designs become necessary when SUTVA (Assumption~\ref{assump:sutva}) is violated. Identification requires either cluster randomisation or explicit modelling of spillovers. Cluster designs assign treatment to groups of units, internalising spillovers within clusters while maintaining independence across clusters. When randomisation is infeasible, spatial models and exposure mappings estimate direct and spillover effects jointly, given network or geographic structure. Chapter~\ref{ch:spillovers} also discusses partial identification when the spillover structure is uncertain.

\subsubsection*{Method Selection Summary}

Method choice depends on the structure of treatment variation, the plausibility of assumptions, and available diagnostics. With staggered timing and plausible parallel trends, use modern DiD (Chapters~\ref{ch:did}, \ref{ch:event}). With one or few treated units, use synthetic control (Chapters~\ref{ch:sc}, \ref{ch:generalized-sc}). With continuous treatments and rich covariates, use unconfoundedness with high-dimensional controls (Chapters~\ref{ch:high-dim}, \ref{ch:ml-nuisance}). With spillovers, use interference-aware designs (Chapter~\ref{ch:spillovers}). Comparing multiple approaches is often most persuasive.

Credible causal inference requires not just sophisticated estimators but also transparent articulation of assumptions and rigorous diagnostics to assess their plausibility. This book prioritises identification arguments over estimation mechanics, though we provide detailed coverage of both.

With estimands defined and identification strategies mapped, we now turn to the mechanics: how standard panel regressions implement these ideas, and where they can go wrong.
